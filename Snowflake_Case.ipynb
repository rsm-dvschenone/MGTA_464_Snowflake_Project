{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# Find and load the nearest .env (project root)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "def need(name):\n",
    "    v = os.getenv(name)\n",
    "    if not v:\n",
    "        raise RuntimeError(f\"Missing required env var: {name}\")\n",
    "    # remove accidental quotes/spaces\n",
    "    return v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "SNOWFLAKE_USER     = need(\"SNOWFLAKE_USER\")\n",
    "SNOWFLAKE_PASSWORD = need(\"SNOWFLAKE_PASSWORD\")\n",
    "SNOWFLAKE_ACCOUNT  = need(\"SNOWFLAKE_ACCOUNT\")  # e.g., afaypkh-rjb48354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x71dbc0f71d60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS my_first_warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x71dbc0f71d60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS testdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 13\n",
      "\n",
      "Sample rows:\n",
      "(1, 'A Datum Corporation', '(847) 555-0100', 'http://www.adatum.com', None, None)\n",
      "(2, 'Contoso, Ltd.', '(360) 555-0100', 'http://www.contoso.com', None, None)\n",
      "(3, 'Consolidated Messenger', '(415) 555-0100', 'http://www.consolidatedmessenger.com', None, None)\n",
      "(4, 'Fabrikam, Inc.', '(203) 555-0104', 'http://www.fabrikam.com', None, None)\n",
      "(5, 'Graphic Design Institute', '(406) 555-0105', 'http://www.graphicdesigninstitute.com', None, None)\n",
      "(6, 'Humongous Insurance', '(423) 555-0105', 'http://www.humongousinsurance.com', None, None)\n",
      "(7, 'Litware, Inc.', '(209) 555-0108', 'http://www.litwareinc.com', None, None)\n",
      "(8, 'Lucerne Publishing', '(423) 555-0103', 'http://www.lucernepublishing.com', None, None)\n",
      "(9, 'Nod Publishers', '(252) 555-0100', 'http://www.nodpublishers.com', None, None)\n",
      "(10, 'Northwind Electric Cars', '(201) 555-0105', 'http://www.northwindelectriccars.com', None, None)\n",
      "\n",
      "Schema:\n",
      "SUPPLIERID NUMBER(38,0)\n",
      "SUPPLIERNAME VARCHAR(16777216)\n",
      "SUPPLIERCATEGORYID NUMBER(38,0)\n",
      "PRIMARYCONTACTPERSONID NUMBER(38,0)\n",
      "ALTERNATECONTACTPERSONID NUMBER(38,0)\n",
      "DELIVERYMETHODID NUMBER(38,0)\n",
      "POSTALCITYID NUMBER(38,0)\n",
      "SUPPLIERREFERENCE VARCHAR(16777216)\n",
      "BANKACCOUNTNAME VARCHAR(16777216)\n",
      "BANKACCOUNTBRANCH VARCHAR(16777216)\n",
      "BANKACCOUNTCODE NUMBER(38,0)\n",
      "BANKACCOUNTNUMBER NUMBER(38,0)\n",
      "BANKINTERNATIONALCODE NUMBER(38,0)\n",
      "PAYMENTDAYS NUMBER(38,0)\n",
      "INTERNALCOMMENTS VARCHAR(16777216)\n",
      "PHONENUMBER VARCHAR(16777216)\n",
      "FAXNUMBER VARCHAR(16777216)\n",
      "WEBSITEURL VARCHAR(16777216)\n",
      "DELIVERYADDRESSLINE1 VARCHAR(16777216)\n",
      "DELIVERYADDRESSLINE2 VARCHAR(16777216)\n",
      "DELIVERYPOSTALCODE NUMBER(38,0)\n",
      "DELIVERYLOCATION VARCHAR(16777216)\n",
      "POSTALADDRESSLINE1 VARCHAR(16777216)\n",
      "POSTALADDRESSLINE2 VARCHAR(16777216)\n",
      "POSTALPOSTALCODE NUMBER(38,0)\n",
      "LASTEDITEDBY NUMBER(38,0)\n",
      "VALIDFROM VARCHAR(16777216)\n",
      "VALIDTO VARCHAR(16777216)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ---- 0) connect  ----\n",
    "conn = snowflake.connector.connect(\n",
    "    host=\"qbmhuza-bnb86629.snowflakecomputing.com\",   \n",
    "    account=\"qbmhuza-bnb86629\",                        \n",
    "    user=\"second2\",\n",
    "    password=\"gyczeg6kaHqywownor\",\n",
    "    warehouse=\"COMPUTE_WH\",                           \n",
    ")\n",
    "cs = conn.cursor()\n",
    "\n",
    "# make sure the warehouse will wake itself\n",
    "cs.execute(\"ALTER WAREHOUSE COMPUTE_WH SET AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "\n",
    "# ---- 1) set DB/Schema (use your existing TESTDB) ----\n",
    "DB, SCHEMA = \"TESTDB\", \"PUBLIC\"\n",
    "cs.execute(f\"CREATE DATABASE IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA   IF NOT EXISTS {DB}.{SCHEMA}\")\n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "# ---- 2) read and run your .pgsql file from the repo ----\n",
    "sql_path = Path(\"Data/supplier_case.pgsql\")     \n",
    "assert sql_path.exists(), f\"Not found: {sql_path.resolve()}\"\n",
    "txt = sql_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# tiny Postgres -> Snowflake cleanups\n",
    "txt = \"\\n\".join(l for l in txt.splitlines() if not l.strip().startswith(\"\\\\\"))            # drop psql meta commands\n",
    "txt = re.sub(r\"\\bNUMERIC\\b\", \"NUMBER\", txt, flags=re.I)                                   # NUMERIC -> NUMBER (safe)\n",
    "txt = re.sub(r\"\\bsupplier_case\\b\", f\"{DB}.{SCHEMA}.SUPPLIER_CASE\", txt, flags=re.I)       # fully-qualify table\n",
    "\n",
    "# split on semicolons and execute\n",
    "stmts = [s.strip() for s in re.split(r\";\\s*(?=\\n|$)\", txt) if s.strip()]\n",
    "for s in stmts:\n",
    "    cs.execute(s)\n",
    "\n",
    "# ---- 3) visualize (still only cs.execute) ----\n",
    "print(\"Rows:\", cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SCHEMA}.SUPPLIER_CASE\").fetchone()[0])\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "for r in cs.execute(f\"\"\"\n",
    "    SELECT SupplierID, SupplierName, PhoneNumber, WebsiteURL,\n",
    "           TRY_TO_DATE(ValidFrom) AS ValidFrom, TRY_TO_DATE(ValidTo) AS ValidTo\n",
    "    FROM {DB}.{SCHEMA}.SUPPLIER_CASE\n",
    "    ORDER BY SupplierID\n",
    "    LIMIT 10\n",
    "\"\"\").fetchall():\n",
    "    print(r)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "for r in cs.execute(f\"DESCRIBE TABLE {DB}.{SCHEMA}.SUPPLIER_CASE\").fetchall():\n",
    "    print(r[0], r[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x71dbc0d37080>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE DATABASE TESTDB\")\n",
    "cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN AS\n",
    "SELECT\n",
    "  CAST(SUPPLIERID               AS INT)        AS SUPPLIERID,\n",
    "  SUPPLIERNAME                                   AS SUPPLIERNAME,\n",
    "  CAST(SUPPLIERCATEGORYID       AS INT)        AS SUPPLIERCATEGORYID,\n",
    "  CAST(PRIMARYCONTACTPERSONID   AS INT)        AS PRIMARYCONTACTPERSONID,\n",
    "  CAST(ALTERNATECONTACTPERSONID AS INT)        AS ALTERNATECONTACTPERSONID,\n",
    "  CAST(DELIVERYMETHODID         AS INT)        AS DELIVERYMETHODID,\n",
    "  CAST(POSTALCITYID             AS INT)        AS POSTALCITYID,\n",
    "  SUPPLIERREFERENCE                              AS SUPPLIERREFERENCE,\n",
    "  PHONENUMBER                                   AS PHONENUMBER,\n",
    "  WEBSITEURL                                    AS WEBSITEURL,\n",
    "  DELIVERYADDRESSLINE1                           AS DELIVERYADDRESSLINE1,\n",
    "  CAST(DELIVERYPOSTALCODE       AS INT)        AS DELIVERYPOSTALCODE,\n",
    "  POSTALADDRESSLINE1                             AS POSTALADDRESSLINE1,\n",
    "  CAST(POSTALPOSTALCODE         AS INT)        AS POSTALPOSTALCODE,\n",
    "  CAST(LASTEDITEDBY             AS INT)        AS LASTEDITEDBY,\n",
    "  TRY_TO_DATE(VALIDFROM)                        AS VALIDFROM,\n",
    "  TRY_TO_DATE(VALIDTO)                          AS VALIDTO\n",
    "FROM TESTDB.PUBLIC.SUPPLIER_CASE;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "(1, 'A Datum Corporation', '(847) 555-0100', 'http://www.adatum.com', None, None)\n",
      "(2, 'Contoso, Ltd.', '(360) 555-0100', 'http://www.contoso.com', None, None)\n",
      "(3, 'Consolidated Messenger', '(415) 555-0100', 'http://www.consolidatedmessenger.com', None, None)\n",
      "(4, 'Fabrikam, Inc.', '(203) 555-0104', 'http://www.fabrikam.com', None, None)\n",
      "(5, 'Graphic Design Institute', '(406) 555-0105', 'http://www.graphicdesigninstitute.com', None, None)\n",
      "(6, 'Humongous Insurance', '(423) 555-0105', 'http://www.humongousinsurance.com', None, None)\n",
      "(7, 'Litware, Inc.', '(209) 555-0108', 'http://www.litwareinc.com', None, None)\n",
      "(8, 'Lucerne Publishing', '(423) 555-0103', 'http://www.lucernepublishing.com', None, None)\n",
      "(9, 'Nod Publishers', '(252) 555-0100', 'http://www.nodpublishers.com', None, None)\n",
      "(10, 'Northwind Electric Cars', '(201) 555-0105', 'http://www.northwindelectriccars.com', None, None)\n"
     ]
    }
   ],
   "source": [
    "print(cs.execute(\"SELECT COUNT(*) FROM TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN\").fetchone()[0])\n",
    "for r in cs.execute(\"\"\"\n",
    "  SELECT SUPPLIERID, SUPPLIERNAME, PHONENUMBER, WEBSITEURL, VALIDFROM, VALIDTO\n",
    "  FROM TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN\n",
    "  ORDER BY SUPPLIERID\n",
    "  LIMIT 10\n",
    "\"\"\").fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES ===\n",
      "Columns: NOAA_WEATHER_STATION_ID, VARIABLE, VARIABLE_NAME, DATE, DATETIME, VALUE, UNIT\n",
      " 1: ('CA007011309', 'precipitation', 'Precipitation', datetime.date(2007, 4, 22), datetime.datetime(2007, 4, 22, 0, 0), Decimal('0.000000'), 'Millimeters')\n",
      " 2: ('USC00046826', 'maximum_temperature', 'Maximum Temperature', datetime.date(2002, 10, 21), datetime.datetime(2002, 10, 21, 16, 0), Decimal('23.900000'), 'Degrees Celsius')\n",
      " 3: ('USC00426708', 'temperature_at_observation_time', 'Temperature at Observation Time', datetime.date(2002, 11, 18), datetime.datetime(2002, 11, 18, 19, 0), Decimal('3.300000'), 'Degrees Celsius')\n",
      " 4: ('USR0000ISPA', 'average_temperature', 'Average Temperature', datetime.date(2016, 4, 3), datetime.datetime(2016, 4, 3, 0, 0), Decimal('11.900000'), 'Degrees Celsius')\n",
      " 5: ('CA001018620', 'precipitation', 'Precipitation', datetime.date(2010, 12, 9), datetime.datetime(2010, 12, 9, 0, 0), Decimal('15.800000'), 'Millimeters')\n",
      "Total rows: 952496604\n",
      "\n",
      "=== WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX ===\n",
      "Columns: NOAA_WEATHER_STATION_ID, NOAA_WEATHER_STATION_NAME, COUNTRY_GEO_ID, COUNTRY_NAME, STATE_GEO_ID, STATE_NAME, ZIP_GEO_ID, ZIP_NAME, LATITUDE, LONGITUDE, ELEVATION, WEATHER_STATION_NETWORK, ASSOCIATED_NETWORKS, WORLD_METEOROLOGICAL_ORGANIZATION_ID, SOURCE_DATA\n",
      " 1: ('CA004060983', 'BUFFALO NARROWS (AUT)', 'country/CAN', 'Canada', 'wikidataId/Q1989', 'Saskatchewan', None, None, 55.8333, -108.4167, 440.0, 'Unspecified', None, '71077', '[\\n  \"Environment Canada\",\\n  \"Global Summary of the Day (NCDC DSI-9618)\"\\n]')\n",
      " 2: ('RSM00022550', 'ARHANGELSK', 'country/RUS', 'Russia', None, None, None, None, 64.5, 40.7331, 8.0, 'World Meteorological Organization ID', '[\\n  \"GSN\"\\n]', '22550', '[\\n  \"European Climate Assessment and Dataset (Klein Tank et al., 2002)\",\\n  \"All-Russian Research Institute of Hydrometeorological Information-World Data Center\",\\n  \"Global Summary of the Day (NCDC DSI-9618)\"\\n]')\n",
      " 3: ('UKM00003091', 'CRAIBSTONE', 'country/GBR', 'United Kingdom', None, None, None, None, 57.18, -2.2, 102.0, 'World Meteorological Organization ID', None, '03091', '[\\n  \"European Climate Assessment and Dataset (Klein Tank et al., 2002)\",\\n  \"Global Summary of the Day (NCDC DSI-9618)\"\\n]')\n",
      " 4: ('UPM00034300', 'KHARKIV', 'country/UKR', 'Ukraine', None, None, None, None, 49.9667, 36.1331, 155.0, 'World Meteorological Organization ID', None, '34300', '[\\n  \"All-Russian Research Institute of Hydrometeorological Information-World Data Center\",\\n  \"Global Summary of the Day (NCDC DSI-9618)\",\\n  \"Ukraine update\"\\n]')\n",
      " 5: ('USC00031310', 'CENTER RIDGE 3 S', 'country/USA', 'United States', 'geoId/05', 'Arkansas', 'zip/72027', '72027', 35.3353, -92.5653, 205.7, 'U.S. Cooperative Network ID', None, None, '[\\n  \"High Plains Regional Climate Center real-time data\",\\n  \"U.S. Cooperative Summary of the Day (NCDC DSI-3200)\",\\n  \"U.S. Cooperative Summary of the Day data digitized from paper observer forms (from 2011 to present)\",\\n  \"U.S. Cooperative Summary of the Day -- Transmitted via WxCoder3 (NCDC DSI-3207)\"\\n]')\n",
      "Total rows: 87377\n"
     ]
    }
   ],
   "source": [
    "# --- Add this to ETL_Snowflake.py (after your existing connection code) ---\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "def use_first_existing_db(cs, candidate_names):\n",
    "    \"\"\"\n",
    "    Try each candidate database name; return the first one that exists.\n",
    "    Falls back to SHOW DATABASES LIKE 'WEATHER%ENVIRONMENT%' if none match exactly.\n",
    "    \"\"\"\n",
    "    # Try exact names first\n",
    "    for db in candidate_names:\n",
    "        cs.execute(f\"SHOW DATABASES LIKE '{db}'\")\n",
    "        if cs.fetchone():\n",
    "            return db\n",
    "\n",
    "    # Broad match as a fallback (handles custom naming)\n",
    "    cs.execute(\"SHOW DATABASES LIKE 'WEATHER%ENVIRONMENT%'\")\n",
    "    row = cs.fetchone()\n",
    "    if row:\n",
    "        # SHOW DATABASES returns tuples; the 2nd field is the database NAME\n",
    "        # Typical order: created_on, name, is_default, ...\n",
    "        return row[1]\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Could not find a WEATHER...ENVIRONMENT database. \"\n",
    "        \"Verify the Snowflake Marketplace subscription/name in the web UI.\"\n",
    "    )\n",
    "\n",
    "def print_table_sample(cs, fully_qualified_table, sample_rows=5):\n",
    "    \"\"\"\n",
    "    Print column names, a small sample, and total row count for the given table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {fully_qualified_table} ===\")\n",
    "\n",
    "    # Sample rows\n",
    "    cs.execute(f\"SELECT * FROM {fully_qualified_table} LIMIT {sample_rows}\")\n",
    "    rows = cs.fetchall()\n",
    "    col_names = [desc[0] for desc in cs.description]\n",
    "    print(\"Columns:\", \", \".join(col_names))\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        print(f\"{i:>2}: {r}\")\n",
    "\n",
    "    # Row count (fast metadata-style count is not available; run COUNT(*))\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {fully_qualified_table}\")\n",
    "    total = cs.fetchone()[0]\n",
    "    print(f\"Total rows: {total}\")\n",
    "\n",
    "def print_cybersyn_weather_tables(conn, warehouse=\"COMPUTE_WH\"):\n",
    "    \"\"\"\n",
    "    Uses only the Snowflake Python connector to:\n",
    "    - set the warehouse,\n",
    "    - locate the Marketplace database,\n",
    "    - switch to CYBERSYN schema,\n",
    "    - and print samples + counts from the two NOAA tables.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cs:\n",
    "        # Ensure a warehouse is set (you created MY_FIRST_WAREHOUSE earlier in your file)\n",
    "        cs.execute(f\"USE WAREHOUSE {warehouse}\")\n",
    "\n",
    "        # Part 2, step 7 database names: assignment shows WEATHER__ENVIRONMENT (double underscore),\n",
    "        # but some accounts show WEATHER_ENVIRONMENT (single underscore).\n",
    "        candidate_dbs = [\"WEATHER__ENVIRONMENT\", \"WEATHER_ENVIRONMENT\"]\n",
    "        db_name = use_first_existing_db(cs, candidate_dbs)\n",
    "        cs.execute(f\"USE DATABASE {db_name}\")\n",
    "\n",
    "        # Schema from the brief is CYBERSYN\n",
    "        cs.execute(\"USE SCHEMA CYBERSYN\")\n",
    "\n",
    "        # Fully qualified table names (no quotes needed since identifiers are simple/upper)\n",
    "        tables = [\n",
    "            \"NOAA_WEATHER_METRICS_TIMESERIES\",\n",
    "            \"NOAA_WEATHER_STATION_INDEX\",\n",
    "        ]\n",
    "        for t in tables:\n",
    "            fqtn = f\"{db_name}.CYBERSYN.{t}\"\n",
    "            print_table_sample(cs, fqtn, sample_rows=5)\n",
    "\n",
    "\n",
    "# --- Run it ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Reuse your existing `conn` from above in ETL_Snowflake.py\n",
    "    # Example assumes you already did:\n",
    "    # conn = snowflake.connector.connect(user=..., password=..., account=...)\n",
    "    try:\n",
    "        print_cybersyn_weather_tables(conn, warehouse=\"COMPUTE_WH\")\n",
    "    finally:\n",
    "        try:\n",
    "            conn.close()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    ")\n",
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x71dc9c90bb60>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data\n",
      "Matched CSVs: 41\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-1.csv\n",
      "Staged objects (top 10): [('po_data_stage/2019-1.csv.gz', 6784, '34fcd350f502b9b5e0583ffa3c312efa', 'Thu, 11 Sep 2025 04:58:28 GMT'), ('po_data_stage/2019-10.csv.gz', 3328, '2c657ed5f8fddd94fb37b71ed73d81f9', 'Thu, 11 Sep 2025 04:58:18 GMT'), ('po_data_stage/2019-11.csv.gz', 3056, '5982ab5f5f417ac2bf8ef3185a0dd1df', 'Thu, 11 Sep 2025 04:58:20 GMT'), ('po_data_stage/2019-12.csv.gz', 2992, '81f6ce02a5c4ba06b49f9fcd3472a7ee', 'Thu, 11 Sep 2025 04:58:26 GMT'), ('po_data_stage/2019-2.csv.gz', 2384, '0e310420ca0426a08ea49e6a3f9e4070', 'Thu, 11 Sep 2025 04:58:18 GMT'), ('po_data_stage/2019-3.csv.gz', 2848, 'ff7383038c32efec8a785a5c42dd16b1', 'Thu, 11 Sep 2025 04:58:19 GMT'), ('po_data_stage/2019-4.csv.gz', 3088, '58a4b27a34b078a22f7cc3b8f7dfef15', 'Thu, 11 Sep 2025 04:58:21 GMT'), ('po_data_stage/2019-5.csv.gz', 3200, 'c85d287c6ca3a4a669626a74137bd7d2', 'Thu, 11 Sep 2025 04:58:19 GMT'), ('po_data_stage/2019-6.csv.gz', 3024, 'd9c867526730b1ea93751b9d22dc105a', 'Thu, 11 Sep 2025 04:58:13 GMT'), ('po_data_stage/2019-7.csv.gz', 3264, '78389e2bed1d5de11dd4e4d45de1b88d', 'Thu, 11 Sep 2025 04:58:24 GMT')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x71dc9c90bb60>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "\n",
    "## Creating the PO_Table with Datatypes \n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA}\")\n",
    "cs.execute(\n",
    "\"CREATE OR REPLACE TABLE PO_Data(\"\n",
    "\"purchaseorderid NUMBER(38,0), \"\n",
    "\"supplierid NUMBER(38,0), \"\n",
    "\"orderdate DATE, \"\n",
    "\"deliverymethodid NUMBER(38,0), \"\n",
    "\"contactpersonid NUMBER(38,0), \"\n",
    "\"expecteddeliverydate DATE, \"\n",
    "\"supplierreference VARCHAR, \"\n",
    "\"isorderfinalized NUMBER(1,0), \"\n",
    "\"comments VARCHAR, \"\n",
    "\"internalcomments VARCHAR, \"\n",
    "\"lasteditedby NUMBER(38,0), \"\n",
    "\"purchaseorderlineid NUMBER(38,0), \"\n",
    "\"stockitemid NUMBER(38,0), \"\n",
    "\"orderedouters NUMBER(38,0), \"\n",
    "\"description VARCHAR, \"\n",
    "\"receivedouters NUMBER(38,0), \"\n",
    "\"packagetypeid NUMBER(38,0), \"\n",
    "\"expectedunitpriceperouter NUMBER(18,4), \"\n",
    "\"lastreceiptdate DATE, \"\n",
    "\"isorderlinefinalized NUMBER(1,0), \"\n",
    "\"right_lasteditedby NUMBER(38,0), \"\n",
    "\"right_lasteditedwhen TIMESTAMP_NTZ\"\n",
    "\")\")\n",
    "\n",
    "# ---------- Resolve repo-relative data folder ----------\n",
    "def find_monthly_po_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Locate the 'Data/Monthly PO Data' folder relative to the repository.\n",
    "    Works from notebooks or scripts, on Windows/macOS/Linux.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        Path.cwd() / \"Data\" / \"Monthly PO Data\",\n",
    "        Path.cwd() / \"data\" / \"Monthly PO Data\",\n",
    "    ]\n",
    "\n",
    "    # If running from a subfolder, search upward then rglob for the directory\n",
    "    # 1) Walk up to (at most) 5 levels to find a '.git' folder (repo root)\n",
    "    here = Path.cwd()\n",
    "    ups = [here] + list(here.parents)[:5]\n",
    "    repo_roots = [p for p in ups if (p / \".git\").exists()]\n",
    "    roots_to_search = repo_roots[:1] or [here]\n",
    "\n",
    "    for root in roots_to_search:\n",
    "        candidates.append(root / \"Data\" / \"Monthly PO Data\")\n",
    "        candidates.append(root / \"data\" / \"Monthly PO Data\")\n",
    "        # fallback: recursive search for the exact folder name\n",
    "        for p in root.rglob(\"Monthly PO Data\"):\n",
    "            candidates.append(p)\n",
    "\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            # Must contain CSVs to be considered valid\n",
    "            if any(p.glob(\"*.csv\")):\n",
    "                return p\n",
    "\n",
    "    raise SystemExit(\"Could not find 'Data/Monthly PO Data' in this repo. \"\n",
    "                     \"Make sure the data folder exists and contains .csv files.\")\n",
    "\n",
    "local_dir_path = find_monthly_po_dir()\n",
    "local_dir = str(local_dir_path)  # keep your existing code style\n",
    "print(\"Using data folder:\", local_dir)\n",
    "\n",
    "# ---------- Stage + file format ----------\n",
    "\n",
    "\n",
    "cs.execute(\"CREATE OR REPLACE STAGE po_data_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT po_csv_ff\n",
    "  TYPE=CSV\n",
    "  FIELD_DELIMITER=','\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "  SKIP_HEADER=1\n",
    "  TRIM_SPACE=TRUE\n",
    "  EMPTY_FIELD_AS_NULL=TRUE\n",
    "  NULL_IF=('','NULL','null','00:00.0','0:00.0','00:00','0:00')\n",
    "  DATE_FORMAT='AUTO'\n",
    "  TIME_FORMAT='AUTO'\n",
    "  TIMESTAMP_FORMAT='AUTO'\n",
    "\"\"\")\n",
    "\n",
    "# ---------- Local files to stage (repo-relative) ----------\n",
    "\n",
    "pattern = os.path.join(local_dir, \"*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "print(\"Matched CSVs:\", len(files))\n",
    "if not files:\n",
    "    raise SystemExit(f\"No CSVs matched at: {pattern}\")\n",
    "\n",
    "# ---------- PUT files into stage (auto-compress -> .gz) ----------\n",
    "for filepath in files:\n",
    "    base = os.path.basename(filepath)\n",
    "    if \":\" in base:   # skip Windows ADS like ':Zone.Identifier'\n",
    "        continue\n",
    "    abs_path = os.path.abspath(filepath).replace(\"\\\\\", \"/\")   # ensure forward slashes\n",
    "    file_uri = \"file:///\" + abs_path.lstrip(\"/\")              # exactly 3 slashes, no URL-encoding\n",
    "    print(\"PUT ->\", file_uri)\n",
    "    cs.execute(f\"PUT '{file_uri}' @po_data_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "    \n",
    "\n",
    "# --- sanity check what's in the stage ---\n",
    "cs.execute(\"LIST @po_data_stage\")\n",
    "print(\"Staged objects (top 10):\", cs.fetchall()[:10])\n",
    "\n",
    "# --- load into the table (skipping $12 = lasteditedwhen) ---\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PO_Data\n",
    "  FROM (\n",
    "    SELECT\n",
    "      $1  ::NUMBER(38,0)  AS purchaseorderid,\n",
    "      $2  ::NUMBER(38,0)  AS supplierid,\n",
    "      TRY_TO_DATE($3)     AS orderdate,\n",
    "      $4  ::NUMBER(38,0)  AS deliverymethodid,\n",
    "      $5  ::NUMBER(38,0)  AS contactpersonid,\n",
    "      TRY_TO_DATE($6)     AS expecteddeliverydate,\n",
    "      $7                  AS supplierreference,\n",
    "      $8  ::NUMBER(1,0)   AS isorderfinalized,\n",
    "      $9                  AS comments,\n",
    "      $10                 AS internalcomments,\n",
    "      $11 ::NUMBER(38,0)  AS lasteditedby,\n",
    "      /* skip $12 */\n",
    "      $13 ::NUMBER(38,0)  AS purchaseorderlineid,\n",
    "      $14 ::NUMBER(38,0)  AS stockitemid,\n",
    "      $15 ::NUMBER(38,0)  AS orderedouters,\n",
    "      $16                 AS description,\n",
    "      $17 ::NUMBER(38,0)  AS receivedouters,\n",
    "      $18 ::NUMBER(38,0)  AS packagetypeid,\n",
    "      $19 ::NUMBER(18,4)  AS expectedunitpriceperouter,\n",
    "      TRY_TO_DATE($20)    AS lastreceiptdate,\n",
    "      $21 ::NUMBER(1,0)   AS isorderlinefinalized,\n",
    "      $22 ::NUMBER(38,0)  AS right_lasteditedby,\n",
    "      TRY_TO_TIMESTAMP_NTZ($23) AS right_lasteditedwhen\n",
    "    FROM @po_data_stage (FILE_FORMAT => 'po_csv_ff')\n",
    "  )\n",
    "  ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 8367\n",
      "Row count should equal 8367\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"SELECT COUNT(*) FROM PO_Data\")\n",
    "print(\"Row count:\", cs.fetchone()[0])\n",
    "print(\"Row count should equal 8367\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 4, datetime.date(2019, 3, 1), 7, 2, datetime.date(2019, 3, 21), '293092', 1, None, None, 4, 469, 77, 92, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 92, 6, Decimal('84.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(106, 4, datetime.date(2019, 3, 1), 7, 2, datetime.date(2019, 3, 21), '293092', 1, None, None, 4, 470, 78, 127, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 127, 6, Decimal('84.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(106, 4, datetime.date(2019, 3, 1), 7, 2, datetime.date(2019, 3, 21), '293092', 1, None, None, 4, 471, 80, 20, '\"The Gu\" red shirt XML tag t-shirt (White) M', 20, 6, Decimal('84.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(106, 4, datetime.date(2019, 3, 1), 7, 2, datetime.date(2019, 3, 21), '293092', 1, None, None, 4, 472, 86, 74, '\"The Gu\" red shirt XML tag t-shirt (White) 5XL', 74, 6, Decimal('96.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(106, 4, datetime.date(2019, 3, 1), 7, 2, datetime.date(2019, 3, 21), '293092', 1, None, None, 4, 473, 95, 22, '\"The Gu\" red shirt XML tag t-shirt (Black) XL', 22, 6, Decimal('90.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(106, 4, datetime.date(2019, 3, 1), 7, 2, datetime.date(2019, 3, 21), '293092', 1, None, None, 4, 474, 98, 97, '\"The Gu\" red shirt XML tag t-shirt (Black) 4XL', 97, 6, Decimal('96.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(107, 7, datetime.date(2019, 3, 1), 2, 2, datetime.date(2019, 3, 21), 'BC0280982', 1, None, None, 4, 475, 204, 23, 'Tape dispenser (Red)', 23, 7, Decimal('170.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(108, 4, datetime.date(2019, 3, 2), 7, 2, datetime.date(2019, 3, 9), '293092', 1, None, None, 4, 476, 77, 6, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 6, 6, Decimal('84.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(108, 4, datetime.date(2019, 3, 2), 7, 2, datetime.date(2019, 3, 9), '293092', 1, None, None, 4, 477, 78, 1, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 1, 6, Decimal('84.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n",
      "(108, 4, datetime.date(2019, 3, 2), 7, 2, datetime.date(2019, 3, 9), '293092', 1, None, None, 4, 478, 80, 15, '\"The Gu\" red shirt XML tag t-shirt (White) M', 15, 6, Decimal('84.0000'), datetime.date(2019, 3, 4), 1, 4, None)\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"SELECT * FROM PO_Data LIMIT 10\")\n",
    "for row in cs.fetchall():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) PO totals (POAmount) and a tidy PO header table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x71dc9c90bb60>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One row per purchase order with the required total\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PO_Header AS\n",
    "SELECT\n",
    "  purchaseorderid,\n",
    "  MIN(orderdate)                 AS orderdate,\n",
    "  MIN(supplierid)                AS supplierid,\n",
    "  SUM(receivedouters * expectedunitpriceperouter) AS POAmount\n",
    "FROM PO_Data\n",
    "GROUP BY purchaseorderid;\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

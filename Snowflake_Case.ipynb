{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting snowflake-connector-python<4,>=3\n",
      "  Using cached snowflake_connector_python-3.17.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4,>=3)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python<4,>=3)\n",
      "  Using cached boto3-1.40.30-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (1.35.90)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (43.0.3)\n",
      "Collecting pyOpenSSL<26.0.0,>=22.0.0 (from snowflake-connector-python<4,>=3)\n",
      "  Downloading pyopenssl-25.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2.9.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2.32.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (24.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (4.12.2)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (3.16.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (4.3.6)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (0.13.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4,>=3) (2.22)\n",
      "Collecting cryptography>=3.1.0 (from snowflake-connector-python<4,>=3)\n",
      "  Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0->snowflake-connector-python<4,>=3) (2.2.3)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python<4,>=3)\n",
      "  Using cached botocore-1.40.30-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python<4,>=3) (1.0.1)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.24->snowflake-connector-python<4,>=3)\n",
      "  Using cached s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python<4,>=3) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python<4,>=3) (1.16.0)\n",
      "Using cached snowflake_connector_python-3.17.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading pyopenssl-25.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m784.9 kB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached boto3-1.40.30-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.40.30-py3-none-any.whl (14.0 MB)\n",
      "Using cached s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: asn1crypto, cryptography, botocore, s3transfer, pyOpenSSL, boto3, snowflake-connector-python\n",
      "\u001b[2K  Attempting uninstall: cryptography\n",
      "\u001b[2K    Found existing installation: cryptography 43.0.3\n",
      "\u001b[2K    Uninstalling cryptography-43.0.3:\n",
      "\u001b[2K      Successfully uninstalled cryptography-43.0.3━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K  Attempting uninstall: botocore━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K    Found existing installation: botocore 1.35.90━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling botocore-1.35.90:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.35.90━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: s3transfer[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: s3transfer 0.10.4━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling s3transfer-0.10.4:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [s3transfer]\n",
      "\u001b[2K      Successfully uninstalled s3transfer-0.10.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [s3transfer]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [snowflake-connector-python]-connector-python]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.36.31 requires botocore==1.35.90, but you have botocore 1.40.30 which is incompatible.\n",
      "awscli 1.36.31 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asn1crypto-1.5.1 boto3-1.40.30 botocore-1.40.30 cryptography-45.0.7 pyOpenSSL-25.2.0 s3transfer-0.14.0 snowflake-connector-python-3.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install \"snowflake-connector-python>=3,<4\" python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up Snowflake Credientials from .env\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# Find and load the nearest .env (project root)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "def need(name):\n",
    "    v = os.getenv(name)\n",
    "    if not v:\n",
    "        raise RuntimeError(f\"Missing required env var: {name}\")\n",
    "    # remove accidental quotes/spaces\n",
    "    return v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "SNOWFLAKE_USER     = need(\"SNOWFLAKE_USER\")\n",
    "SNOWFLAKE_PASSWORD = need(\"SNOWFLAKE_PASSWORD\")\n",
    "SNOWFLAKE_ACCOUNT  = need(\"SNOWFLAKE_ACCOUNT\")  # e.g., afaypkh-rjb48354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Snowflake Connector\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd1002fa70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Creating Warehouse\n",
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS my_first_warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd1002fa70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Creating Testdb\n",
    "\n",
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS testdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 13\n",
      "\n",
      "Sample rows:\n",
      "(1, 'A Datum Corporation', '(847) 555-0100', 'http://www.adatum.com', None, None)\n",
      "(2, 'Contoso, Ltd.', '(360) 555-0100', 'http://www.contoso.com', None, None)\n",
      "(3, 'Consolidated Messenger', '(415) 555-0100', 'http://www.consolidatedmessenger.com', None, None)\n",
      "(4, 'Fabrikam, Inc.', '(203) 555-0104', 'http://www.fabrikam.com', None, None)\n",
      "(5, 'Graphic Design Institute', '(406) 555-0105', 'http://www.graphicdesigninstitute.com', None, None)\n",
      "(6, 'Humongous Insurance', '(423) 555-0105', 'http://www.humongousinsurance.com', None, None)\n",
      "(7, 'Litware, Inc.', '(209) 555-0108', 'http://www.litwareinc.com', None, None)\n",
      "(8, 'Lucerne Publishing', '(423) 555-0103', 'http://www.lucernepublishing.com', None, None)\n",
      "(9, 'Nod Publishers', '(252) 555-0100', 'http://www.nodpublishers.com', None, None)\n",
      "(10, 'Northwind Electric Cars', '(201) 555-0105', 'http://www.northwindelectriccars.com', None, None)\n",
      "\n",
      "Schema:\n",
      "SUPPLIERID NUMBER(38,0)\n",
      "SUPPLIERNAME VARCHAR(16777216)\n",
      "SUPPLIERCATEGORYID NUMBER(38,0)\n",
      "PRIMARYCONTACTPERSONID NUMBER(38,0)\n",
      "ALTERNATECONTACTPERSONID NUMBER(38,0)\n",
      "DELIVERYMETHODID NUMBER(38,0)\n",
      "POSTALCITYID NUMBER(38,0)\n",
      "SUPPLIERREFERENCE VARCHAR(16777216)\n",
      "BANKACCOUNTNAME VARCHAR(16777216)\n",
      "BANKACCOUNTBRANCH VARCHAR(16777216)\n",
      "BANKACCOUNTCODE NUMBER(38,0)\n",
      "BANKACCOUNTNUMBER NUMBER(38,0)\n",
      "BANKINTERNATIONALCODE NUMBER(38,0)\n",
      "PAYMENTDAYS NUMBER(38,0)\n",
      "INTERNALCOMMENTS VARCHAR(16777216)\n",
      "PHONENUMBER VARCHAR(16777216)\n",
      "FAXNUMBER VARCHAR(16777216)\n",
      "WEBSITEURL VARCHAR(16777216)\n",
      "DELIVERYADDRESSLINE1 VARCHAR(16777216)\n",
      "DELIVERYADDRESSLINE2 VARCHAR(16777216)\n",
      "DELIVERYPOSTALCODE NUMBER(38,0)\n",
      "DELIVERYLOCATION VARCHAR(16777216)\n",
      "POSTALADDRESSLINE1 VARCHAR(16777216)\n",
      "POSTALADDRESSLINE2 VARCHAR(16777216)\n",
      "POSTALPOSTALCODE NUMBER(38,0)\n",
      "LASTEDITEDBY NUMBER(38,0)\n",
      "VALIDFROM VARCHAR(16777216)\n",
      "VALIDTO VARCHAR(16777216)\n"
     ]
    }
   ],
   "source": [
    "##Creating Suupplier_Case info \n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ---- 0) connect  ----\n",
    "conn = snowflake.connector.connect(\n",
    "    host=\"qbmhuza-bnb86629.snowflakecomputing.com\",   \n",
    "    account=\"qbmhuza-bnb86629\",                        \n",
    "    user=\"second2\",\n",
    "    password=\"gyczeg6kaHqywownor\",\n",
    "    warehouse=\"COMPUTE_WH\",                           \n",
    ")\n",
    "cs = conn.cursor()\n",
    "\n",
    "# make sure the warehouse will wake itself\n",
    "cs.execute(\"ALTER WAREHOUSE COMPUTE_WH SET AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "\n",
    "# ---- 1) set DB/Schema (use your existing TESTDB) ----\n",
    "DB, SCHEMA = \"TESTDB\", \"PUBLIC\"\n",
    "cs.execute(f\"CREATE DATABASE IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA   IF NOT EXISTS {DB}.{SCHEMA}\")\n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "# ---- 2) read and run your .pgsql file from the repo ----\n",
    "sql_path = Path(\"Data/supplier_case.pgsql\")     \n",
    "assert sql_path.exists(), f\"Not found: {sql_path.resolve()}\"\n",
    "txt = sql_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# tiny Postgres -> Snowflake cleanups\n",
    "txt = \"\\n\".join(l for l in txt.splitlines() if not l.strip().startswith(\"\\\\\"))            # drop psql meta commands\n",
    "txt = re.sub(r\"\\bNUMERIC\\b\", \"NUMBER\", txt, flags=re.I)                                   # NUMERIC -> NUMBER (safe)\n",
    "txt = re.sub(r\"\\bsupplier_case\\b\", f\"{DB}.{SCHEMA}.SUPPLIER_CASE\", txt, flags=re.I)       # fully-qualify table\n",
    "\n",
    "# split on semicolons and execute\n",
    "stmts = [s.strip() for s in re.split(r\";\\s*(?=\\n|$)\", txt) if s.strip()]\n",
    "for s in stmts:\n",
    "    cs.execute(s)\n",
    "\n",
    "# ---- 3) visualize (still only cs.execute) ----\n",
    "print(\"Rows:\", cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SCHEMA}.SUPPLIER_CASE\").fetchone()[0])\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "for r in cs.execute(f\"\"\"\n",
    "    SELECT SupplierID, SupplierName, PhoneNumber, WebsiteURL,\n",
    "           TRY_TO_DATE(ValidFrom) AS ValidFrom, TRY_TO_DATE(ValidTo) AS ValidTo\n",
    "    FROM {DB}.{SCHEMA}.SUPPLIER_CASE\n",
    "    ORDER BY SupplierID\n",
    "    LIMIT 10\n",
    "\"\"\").fetchall():\n",
    "    print(r)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "for r in cs.execute(f\"DESCRIBE TABLE {DB}.{SCHEMA}.SUPPLIER_CASE\").fetchall():\n",
    "    print(r[0], r[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd1002ff80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Setting actua db/schema and making table\n",
    "\n",
    "cs.execute(\"USE DATABASE TESTDB\")\n",
    "cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN AS\n",
    "SELECT\n",
    "  CAST(SUPPLIERID               AS INT)        AS SUPPLIERID,\n",
    "  SUPPLIERNAME                                   AS SUPPLIERNAME,\n",
    "  CAST(SUPPLIERCATEGORYID       AS INT)        AS SUPPLIERCATEGORYID,\n",
    "  CAST(PRIMARYCONTACTPERSONID   AS INT)        AS PRIMARYCONTACTPERSONID,\n",
    "  CAST(ALTERNATECONTACTPERSONID AS INT)        AS ALTERNATECONTACTPERSONID,\n",
    "  CAST(DELIVERYMETHODID         AS INT)        AS DELIVERYMETHODID,\n",
    "  CAST(POSTALCITYID             AS INT)        AS POSTALCITYID,\n",
    "  SUPPLIERREFERENCE                              AS SUPPLIERREFERENCE,\n",
    "  PHONENUMBER                                   AS PHONENUMBER,\n",
    "  WEBSITEURL                                    AS WEBSITEURL,\n",
    "  DELIVERYADDRESSLINE1                           AS DELIVERYADDRESSLINE1,\n",
    "  CAST(DELIVERYPOSTALCODE       AS INT)        AS DELIVERYPOSTALCODE,\n",
    "  POSTALADDRESSLINE1                             AS POSTALADDRESSLINE1,\n",
    "  CAST(POSTALPOSTALCODE         AS INT)        AS POSTALPOSTALCODE,\n",
    "  CAST(LASTEDITEDBY             AS INT)        AS LASTEDITEDBY,\n",
    "  TRY_TO_DATE(VALIDFROM)                        AS VALIDFROM,\n",
    "  TRY_TO_DATE(VALIDTO)                          AS VALIDTO\n",
    "FROM TESTDB.PUBLIC.SUPPLIER_CASE;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "(1, 'A Datum Corporation', '(847) 555-0100', 'http://www.adatum.com', None, None)\n",
      "(2, 'Contoso, Ltd.', '(360) 555-0100', 'http://www.contoso.com', None, None)\n",
      "(3, 'Consolidated Messenger', '(415) 555-0100', 'http://www.consolidatedmessenger.com', None, None)\n",
      "(4, 'Fabrikam, Inc.', '(203) 555-0104', 'http://www.fabrikam.com', None, None)\n",
      "(5, 'Graphic Design Institute', '(406) 555-0105', 'http://www.graphicdesigninstitute.com', None, None)\n",
      "(6, 'Humongous Insurance', '(423) 555-0105', 'http://www.humongousinsurance.com', None, None)\n",
      "(7, 'Litware, Inc.', '(209) 555-0108', 'http://www.litwareinc.com', None, None)\n",
      "(8, 'Lucerne Publishing', '(423) 555-0103', 'http://www.lucernepublishing.com', None, None)\n",
      "(9, 'Nod Publishers', '(252) 555-0100', 'http://www.nodpublishers.com', None, None)\n",
      "(10, 'Northwind Electric Cars', '(201) 555-0105', 'http://www.northwindelectriccars.com', None, None)\n"
     ]
    }
   ],
   "source": [
    "##Testing that it works\n",
    "\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN\").fetchone()[0])\n",
    "for r in cs.execute(\"\"\"\n",
    "  SELECT SUPPLIERID, SUPPLIERNAME, PHONENUMBER, WEBSITEURL, VALIDFROM, VALIDTO\n",
    "  FROM TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN\n",
    "  ORDER BY SUPPLIERID\n",
    "  LIMIT 10\n",
    "\"\"\").fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting weather data from NOAA... Searching first for those specific tables then putting into Snowflake Env\n",
    "import snowflake.connector\n",
    "\n",
    "def find_weather_db(cs):\n",
    "    # Get all db names\n",
    "    names = [r[1] for r in cs.execute(\"SHOW DATABASES\").fetchall()]\n",
    "    # Try exact names from the brief\n",
    "    for cand in (\"WEATHER__ENVIRONMENT\", \"WEATHER_ENVIRONMENT\"):\n",
    "        if cand in names:\n",
    "            return cand\n",
    "    # Fuzzy fallback (handles custom names)\n",
    "    for n in names:\n",
    "        if \"WEATHER\" in n and \"ENVIRONMENT\" in n:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def print_table_sample(cs, fqtn, sample_rows=5):\n",
    "    print(f\"\\n=== {fqtn} ===\")\n",
    "    cs.execute(f\"SELECT * FROM {fqtn} LIMIT {sample_rows}\")\n",
    "    rows = cs.fetchall()\n",
    "    cols = [d[0] for d in cs.description]\n",
    "    print(\"Columns:\", \", \".join(cols))\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        print(f\"{i:>2}: {r}\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {fqtn}\")\n",
    "    print(\"Total rows:\", cs.fetchone()[0])\n",
    "\n",
    "def print_cybersyn_weather_tables(conn, warehouse=\"COMPUTE_WH\"):\n",
    "    with conn.cursor() as cs:\n",
    "        cs.execute(f\"USE WAREHOUSE {warehouse}\")\n",
    "\n",
    "        db_name = find_weather_db(cs)\n",
    "        if not db_name:\n",
    "            print(\"⚠️  Skipping Cybersyn weather: no WEATHER…ENVIRONMENT database found in this account.\")\n",
    "            have = [r[1] for r in cs.execute(\"SHOW DATABASES\").fetchall()]\n",
    "            print(\"Databases you have:\", have)\n",
    "            return\n",
    "\n",
    "        cs.execute(f\"USE DATABASE {db_name}\")\n",
    "\n",
    "        # Prefer CYBERSYN schema if present; otherwise fall back to PUBLIC\n",
    "        schemas = {r[1] for r in cs.execute(f\"SHOW SCHEMAS IN DATABASE {db_name}\").fetchall()}\n",
    "        schema = \"CYBERSYN\" if \"CYBERSYN\" in schemas else \"PUBLIC\"\n",
    "        cs.execute(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "        # If the exact NOAA table names differ, list what’s there and pick two NOAA* tables\n",
    "        all_tables = [r[1] for r in cs.execute(f\"SHOW TABLES IN SCHEMA {db_name}.{schema}\").fetchall()]\n",
    "        candidates = [t for t in all_tables if t.startswith(\"NOAA_\")]\n",
    "        if not candidates:\n",
    "            print(f\"No NOAA_* tables in {db_name}.{schema}. Available tables:\", all_tables)\n",
    "            return\n",
    "\n",
    "        for t in candidates[:2]:\n",
    "            print_table_sample(cs, f\"{db_name}.{schema}.{t}\", sample_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    ")\n",
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data\n",
      "Matched CSVs: 41\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-1.csv\n",
      "Staged objects (top 10): [('po_data_stage/2019-1.csv.gz', 6784, 'a15f2fa77d98a431c564917c46a8f54f', 'Sun, 14 Sep 2025 21:57:56 GMT'), ('po_data_stage/2019-10.csv.gz', 3328, '30fddf0b054c15abdf58d9008cfd507d', 'Sun, 14 Sep 2025 21:57:42 GMT'), ('po_data_stage/2019-11.csv.gz', 3056, 'cea00f90062fda98ac3b9cf93003f446', 'Sun, 14 Sep 2025 21:57:44 GMT'), ('po_data_stage/2019-12.csv.gz', 2992, 'f7f2bf307ed9bc4844dac53cbd45e6e6', 'Sun, 14 Sep 2025 21:57:53 GMT'), ('po_data_stage/2019-2.csv.gz', 2384, 'c63aedaf830d7dba4e7080fcd8b2fd8a', 'Sun, 14 Sep 2025 21:57:40 GMT'), ('po_data_stage/2019-3.csv.gz', 2848, 'dc2ccfb5cad11094f2139c3bd04bfb5e', 'Sun, 14 Sep 2025 21:57:43 GMT'), ('po_data_stage/2019-4.csv.gz', 3088, '8c7471de208492d2a57559e77a1b148d', 'Sun, 14 Sep 2025 21:57:46 GMT'), ('po_data_stage/2019-5.csv.gz', 3200, '8c01985520ab970c77732bd8a2d88a9d', 'Sun, 14 Sep 2025 21:57:44 GMT'), ('po_data_stage/2019-6.csv.gz', 3024, 'f926b36f2e78f5ec00e1b688260f5d9e', 'Sun, 14 Sep 2025 21:57:31 GMT'), ('po_data_stage/2019-7.csv.gz', 3264, 'ec2a86136f0ff8bf13eeeac1ded84faa', 'Sun, 14 Sep 2025 21:57:51 GMT')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd100688f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-creating the PO Table and setting datatypes\n",
    "\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "\n",
    "## Creating the PO_Table with Datatypes \n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA}\")\n",
    "cs.execute(\n",
    "\"CREATE OR REPLACE TABLE PO_Data(\"\n",
    "\"purchaseorderid NUMBER(38,0), \"\n",
    "\"supplierid NUMBER(38,0), \"\n",
    "\"orderdate DATE, \"\n",
    "\"deliverymethodid NUMBER(38,0), \"\n",
    "\"contactpersonid NUMBER(38,0), \"\n",
    "\"expecteddeliverydate DATE, \"\n",
    "\"supplierreference VARCHAR, \"\n",
    "\"isorderfinalized NUMBER(1,0), \"\n",
    "\"comments VARCHAR, \"\n",
    "\"internalcomments VARCHAR, \"\n",
    "\"lasteditedby NUMBER(38,0), \"\n",
    "\"purchaseorderlineid NUMBER(38,0), \"\n",
    "\"stockitemid NUMBER(38,0), \"\n",
    "\"orderedouters NUMBER(38,0), \"\n",
    "\"description VARCHAR, \"\n",
    "\"receivedouters NUMBER(38,0), \"\n",
    "\"packagetypeid NUMBER(38,0), \"\n",
    "\"expectedunitpriceperouter NUMBER(18,4), \"\n",
    "\"lastreceiptdate DATE, \"\n",
    "\"isorderlinefinalized NUMBER(1,0), \"\n",
    "\"right_lasteditedby NUMBER(38,0), \"\n",
    "\"right_lasteditedwhen TIMESTAMP_NTZ\"\n",
    "\")\")\n",
    "\n",
    "# ---------- Resolve repo-relative data folder ----------\n",
    "def find_monthly_po_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Locate the 'Data/Monthly PO Data' folder relative to the repository.\n",
    "    Works from notebooks or scripts, on Windows/macOS/Linux.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        Path.cwd() / \"Data\" / \"Monthly PO Data\",\n",
    "        Path.cwd() / \"data\" / \"Monthly PO Data\",\n",
    "    ]\n",
    "\n",
    "    # If running from a subfolder, search upward then rglob for the directory\n",
    "    # 1) Walk up to (at most) 5 levels to find a '.git' folder (repo root)\n",
    "    here = Path.cwd()\n",
    "    ups = [here] + list(here.parents)[:5]\n",
    "    repo_roots = [p for p in ups if (p / \".git\").exists()]\n",
    "    roots_to_search = repo_roots[:1] or [here]\n",
    "\n",
    "    for root in roots_to_search:\n",
    "        candidates.append(root / \"Data\" / \"Monthly PO Data\")\n",
    "        candidates.append(root / \"data\" / \"Monthly PO Data\")\n",
    "        # fallback: recursive search for the exact folder name\n",
    "        for p in root.rglob(\"Monthly PO Data\"):\n",
    "            candidates.append(p)\n",
    "\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            # Must contain CSVs to be considered valid\n",
    "            if any(p.glob(\"*.csv\")):\n",
    "                return p\n",
    "\n",
    "    raise SystemExit(\"Could not find 'Data/Monthly PO Data' in this repo. \"\n",
    "                     \"Make sure the data folder exists and contains .csv files.\")\n",
    "\n",
    "local_dir_path = find_monthly_po_dir()\n",
    "local_dir = str(local_dir_path)  # keep your existing code style\n",
    "print(\"Using data folder:\", local_dir)\n",
    "\n",
    "# ---------- Stage + file format ----------\n",
    "\n",
    "\n",
    "cs.execute(\"CREATE OR REPLACE STAGE po_data_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT po_csv_ff\n",
    "  TYPE=CSV\n",
    "  FIELD_DELIMITER=','\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "  SKIP_HEADER=1\n",
    "  TRIM_SPACE=TRUE\n",
    "  EMPTY_FIELD_AS_NULL=TRUE\n",
    "  NULL_IF=('','NULL','null','00:00.0','0:00.0','00:00','0:00')\n",
    "  DATE_FORMAT='AUTO'\n",
    "  TIME_FORMAT='AUTO'\n",
    "  TIMESTAMP_FORMAT='AUTO'\n",
    "\"\"\")\n",
    "\n",
    "# ---------- Local files to stage (repo-relative) ----------\n",
    "\n",
    "pattern = os.path.join(local_dir, \"*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "print(\"Matched CSVs:\", len(files))\n",
    "if not files:\n",
    "    raise SystemExit(f\"No CSVs matched at: {pattern}\")\n",
    "\n",
    "# ---------- PUT files into stage (auto-compress -> .gz) ----------\n",
    "for filepath in files:\n",
    "    base = os.path.basename(filepath)\n",
    "    if \":\" in base:   # skip Windows ADS like ':Zone.Identifier'\n",
    "        continue\n",
    "    abs_path = os.path.abspath(filepath).replace(\"\\\\\", \"/\")   # ensure forward slashes\n",
    "    file_uri = \"file:///\" + abs_path.lstrip(\"/\")              # exactly 3 slashes, no URL-encoding\n",
    "    print(\"PUT ->\", file_uri)\n",
    "    cs.execute(f\"PUT '{file_uri}' @po_data_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "    \n",
    "\n",
    "# --- sanity check what's in the stage ---\n",
    "cs.execute(\"LIST @po_data_stage\")\n",
    "print(\"Staged objects (top 10):\", cs.fetchall()[:10])\n",
    "\n",
    "# --- load into the table (skipping $12 = lasteditedwhen) ---\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PO_Data\n",
    "  FROM (\n",
    "    SELECT\n",
    "      $1  ::NUMBER(38,0)  AS purchaseorderid,\n",
    "      $2  ::NUMBER(38,0)  AS supplierid,\n",
    "      TRY_TO_DATE($3)     AS orderdate,\n",
    "      $4  ::NUMBER(38,0)  AS deliverymethodid,\n",
    "      $5  ::NUMBER(38,0)  AS contactpersonid,\n",
    "      TRY_TO_DATE($6)     AS expecteddeliverydate,\n",
    "      $7                  AS supplierreference,\n",
    "      $8  ::NUMBER(1,0)   AS isorderfinalized,\n",
    "      $9                  AS comments,\n",
    "      $10                 AS internalcomments,\n",
    "      $11 ::NUMBER(38,0)  AS lasteditedby,\n",
    "      /* skip $12 */\n",
    "      $13 ::NUMBER(38,0)  AS purchaseorderlineid,\n",
    "      $14 ::NUMBER(38,0)  AS stockitemid,\n",
    "      $15 ::NUMBER(38,0)  AS orderedouters,\n",
    "      $16                 AS description,\n",
    "      $17 ::NUMBER(38,0)  AS receivedouters,\n",
    "      $18 ::NUMBER(38,0)  AS packagetypeid,\n",
    "      $19 ::NUMBER(18,4)  AS expectedunitpriceperouter,\n",
    "      TRY_TO_DATE($20)    AS lastreceiptdate,\n",
    "      $21 ::NUMBER(1,0)   AS isorderlinefinalized,\n",
    "      $22 ::NUMBER(38,0)  AS right_lasteditedby,\n",
    "      TRY_TO_TIMESTAMP_NTZ($23) AS right_lasteditedwhen\n",
    "    FROM @po_data_stage (FILE_FORMAT => 'po_csv_ff')\n",
    "  )\n",
    "  ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 8367\n",
      "Row count should equal 8367\n"
     ]
    }
   ],
   "source": [
    "##Testing that data was created completely \n",
    "\n",
    "cs.execute(\"SELECT COUNT(*) FROM PO_Data\")\n",
    "print(\"Row count:\", cs.fetchone()[0])\n",
    "print(\"Row count should equal 8367\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2160, 77, 767, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 767, 6, Decimal('84.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2161, 78, 981, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 981, 6, Decimal('84.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2162, 80, 397, '\"The Gu\" red shirt XML tag t-shirt (White) M', 397, 6, Decimal('84.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2163, 86, 816, '\"The Gu\" red shirt XML tag t-shirt (White) 5XL', 816, 6, Decimal('96.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2164, 95, 521, '\"The Gu\" red shirt XML tag t-shirt (Black) XL', 521, 6, Decimal('90.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2165, 98, 978, '\"The Gu\" red shirt XML tag t-shirt (Black) 4XL', 978, 6, Decimal('96.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(559, 7, datetime.date(2019, 12, 2), 2, 2, datetime.date(2019, 12, 22), 'BC0280982', 1, None, None, 8, 2166, 193, 344, 'Black and orange glass with care despatch tape 48mmx75m', 344, 7, Decimal('38.4000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(559, 7, datetime.date(2019, 12, 2), 2, 2, datetime.date(2019, 12, 22), 'BC0280982', 1, None, None, 8, 2167, 204, 467, 'Tape dispenser (Red)', 467, 7, Decimal('170.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(560, 4, datetime.date(2019, 12, 3), 7, 2, datetime.date(2019, 12, 23), '293092', 1, None, None, 17, 2168, 77, 769, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 769, 6, Decimal('84.0000'), datetime.date(2019, 12, 4), 1, 17, None)\n",
      "(560, 4, datetime.date(2019, 12, 3), 7, 2, datetime.date(2019, 12, 23), '293092', 1, None, None, 17, 2169, 78, 979, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 979, 6, Decimal('84.0000'), datetime.date(2019, 12, 4), 1, 17, None)\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"SELECT * FROM PO_Data LIMIT 10\")\n",
    "for row in cs.fetchall():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) PO totals (POAmount) and a tidy PO header table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd100688f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One row per purchase order with the required total\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PO_Header AS\n",
    "SELECT\n",
    "  purchaseorderid,\n",
    "  MIN(orderdate)                 AS orderdate,\n",
    "  MIN(supplierid)                AS supplierid,\n",
    "  SUM(receivedouters * expectedunitpriceperouter) AS POAmount\n",
    "FROM PO_Data\n",
    "GROUP BY purchaseorderid;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file URI: file:///home/jovyan/sf_uploads/supplier_transactions.xml\n",
      "[('invoice_xml_stage/supplier_transactions.xml.gz', 72528, '15c0f8f4276bf24ac457fb4e00e4107a', 'Sun, 14 Sep 2025 22:40:45 GMT')]\n"
     ]
    }
   ],
   "source": [
    "##Getting supplier transaction data via XML extraction\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Path to the XML in your repo (adjust this if your folder name differs)\n",
    "repo_xml = Path(\"Data\") / \"Supplier Transactions XML.xml\"\n",
    "\n",
    "# Make a simple, safe upload path that definitely exists *inside the container*\n",
    "safe_dir = Path.home() / \"sf_uploads\"\n",
    "safe_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "safe = safe_dir / \"supplier_transactions.xml\"   # normalize name\n",
    "shutil.copy2(repo_xml, safe)                     # copy into place\n",
    "\n",
    "uri = safe.as_uri()  # e.g., file:///home/jovyan/sf_uploads/supplier_transactions.xml\n",
    "print(\"Local file URI:\", uri)\n",
    "\n",
    "# Make sure the stage exists and is an *internal* stage\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS invoice_xml_stage\")\n",
    "\n",
    "# Do the PUT\n",
    "cs.execute(f\"PUT '{uri}' @invoice_xml_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "\n",
    "# Confirm\n",
    "print(cs.execute(\"LIST @invoice_xml_stage\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml_ff (with STRIP_OUTER_ELEMENT=TRUE) ready.\n"
     ]
    }
   ],
   "source": [
    "# 1) Recreate the XML file format WITH strip_outer_element\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT xml_ff\n",
    "  TYPE=XML\n",
    "  STRIP_OUTER_ELEMENT=TRUE\n",
    "\"\"\")\n",
    "print(\"xml_ff (with STRIP_OUTER_ELEMENT=TRUE) ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOICE_RAW rows (should be ~2438): 2438\n"
     ]
    }
   ],
   "source": [
    "# 2) Reload the XML so each <row> is its own table row\n",
    "cs.execute(\"CREATE OR REPLACE TABLE INVOICE_RAW (doc VARIANT)\")\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO INVOICE_RAW\n",
    "FROM @invoice_xml_stage\n",
    "FILE_FORMAT = xml_ff\n",
    "ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")\n",
    "print(\"INVOICE_RAW rows (should be ~2438):\", cs.execute(\"SELECT COUNT(*) FROM INVOICE_RAW\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in SUPPLIER_INVOICES: 2438\n",
      "[(134, 2, 5, 1, '7290', datetime.date(2019, 1, 2), Decimal('313.50'), Decimal('47.03'), Decimal('360.53'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (169, 4, 5, 2, '3898', datetime.date(2019, 1, 2), Decimal('21732.00'), Decimal('3259.80'), Decimal('24991.80'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (186, 5, 5, 3, '616', datetime.date(2019, 1, 2), Decimal('2740.50'), Decimal('411.11'), Decimal('3151.61'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (215, 7, 5, 4, '3869', datetime.date(2019, 1, 2), Decimal('42481.20'), Decimal('6372.19'), Decimal('48853.39'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (224, 10, 5, 5, '4697', datetime.date(2019, 1, 2), Decimal('35067.50'), Decimal('5260.14'), Decimal('40327.64'), Decimal('0.00'), datetime.date(2019, 1, 7), True)]\n"
     ]
    }
   ],
   "source": [
    "##Taking Invoice_raw staging table and putting it into SUPPLIER_INVOICES in actual Schema\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE SUPPLIER_INVOICES AS\n",
    "SELECT\n",
    "  TRY_TO_NUMBER(XMLGET(doc,'SupplierTransactionID'):\"$\"::STRING)      AS SupplierTransactionID,\n",
    "  TRY_TO_NUMBER(XMLGET(doc,'SupplierID'):\"$\"::STRING)                  AS SupplierID,\n",
    "  TRY_TO_NUMBER(XMLGET(doc,'TransactionTypeID'):\"$\"::STRING)           AS TransactionTypeID,\n",
    "  NULLIF(XMLGET(doc,'PurchaseOrderID'):\"$\"::STRING,'')::NUMBER         AS PurchaseOrderID,\n",
    "  NULLIF(XMLGET(doc,'SupplierInvoiceNumber'):\"$\"::STRING,'')           AS SupplierInvoiceNumber,\n",
    "  TRY_TO_DATE(XMLGET(doc,'TransactionDate'):\"$\"::STRING)               AS TransactionDate,   -- safer\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'AmountExcludingTax'):\"$\"::STRING,18,2)    AS AmountExcludingTax,\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'TaxAmount'):\"$\"::STRING,18,2)             AS TaxAmount,\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'TransactionAmount'):\"$\"::STRING,18,2)     AS TransactionAmount,\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'OutstandingBalance'):\"$\"::STRING,18,2)    AS OutstandingBalance,\n",
    "  TRY_TO_DATE(XMLGET(doc,'FinalizationDate'):\"$\"::STRING)              AS FinalizationDate,\n",
    "  TRY_TO_BOOLEAN(XMLGET(doc,'IsFinalized'):\"$\"::STRING)                AS IsFinalized\n",
    "FROM INVOICE_RAW\n",
    "\"\"\")\n",
    "\n",
    "print(\"Rows in SUPPLIER_INVOICES:\", cs.execute(\"SELECT COUNT(*) FROM SUPPLIER_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM SUPPLIER_INVOICES LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate PO tables: [('PUBLIC', 'PO_DATA')]\n",
      "Chosen PO table: ('PUBLIC', 'PO_DATA') with columns: RECEIVEDOUTERS x EXPECTEDUNITPRICEPEROUTER\n"
     ]
    }
   ],
   "source": [
    "# 1) Find a likely PO table and the columns we need\n",
    "#    (searches current database across common schemas; tweak the schema list if needed)\n",
    "schemas_to_check = [\"STAGE_AND_RAW\", \"PUBLIC\", \"RAW\", \"DATA\", \"MARTS\"]\n",
    "like_filters = [\"%PO%\", \"%PURCHASE%\"]\n",
    "\n",
    "# helper: run a query and return rows\n",
    "def q(sql):\n",
    "    return cs.execute(sql).fetchall()\n",
    "\n",
    "# find candidate tables\n",
    "candidates = []\n",
    "for sch in schemas_to_check:\n",
    "    rows = q(f\"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_type='BASE TABLE'\n",
    "          AND table_schema = '{sch}'\n",
    "          AND (UPPER(table_name) LIKE '{like_filters[0]}' OR UPPER(table_name) LIKE '{like_filters[1]}')\n",
    "    \"\"\")\n",
    "    candidates.extend(rows)\n",
    "\n",
    "print(\"Candidate PO tables:\", candidates)\n",
    "\n",
    "# choose the best candidate that has PurchaseOrderID and a price*qty pair\n",
    "chosen = None\n",
    "qty_col = None\n",
    "price_col = None\n",
    "\n",
    "pairs = [\n",
    "    (\"RECEIVEDOUTERS\", \"EXPECTEDUNITPRICEPEROUTER\"),\n",
    "    (\"QUANTITY\", \"UNITPRICE\")\n",
    "]\n",
    "\n",
    "for sch, tbl in candidates:\n",
    "    cols = {r[0] for r in q(f\"\"\"\n",
    "        SELECT UPPER(column_name)\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{sch}' AND table_name = '{tbl}'\n",
    "    \"\"\")}\n",
    "    if \"PURCHASEORDERID\" in cols:\n",
    "        for qcol, pcol in pairs:\n",
    "            if qcol in cols and pcol in cols:\n",
    "                chosen = (sch, tbl)\n",
    "                qty_col, price_col = qcol, pcol\n",
    "                break\n",
    "    if chosen:\n",
    "        break\n",
    "\n",
    "if not chosen:\n",
    "    raise RuntimeError(\"Couldn't auto-detect a PO detail table with the needed columns. \"\n",
    "                       \"If you already know it, set `chosen=('YOUR_SCHEMA','YOUR_TABLE')` \"\n",
    "                       \"and `qty_col, price_col` accordingly and re-run.\")\n",
    "\n",
    "print(\"Chosen PO table:\", chosen, \"with columns:\", qty_col, \"x\", price_col)\n",
    "\n",
    "# for convenience\n",
    "PO_SCHEMA, PO_TABLE = chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "po_totals sample: [(562, Decimal('402192.0000')), (576, Decimal('407994.0000')), (814, Decimal('186559.6000')), (577, Decimal('98336.4000')), (581, Decimal('413298.0000'))]\n"
     ]
    }
   ],
   "source": [
    "##Creating PO_totals table \n",
    "\n",
    "PO_SCHEMA, PO_TABLE = \"PUBLIC\", \"PO_DATA\"   # from your auto-detect result\n",
    "qty_col, price_col = \"RECEIVEDOUTERS\", \"EXPECTEDUNITPRICEPEROUTER\"\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {PO_SCHEMA}.PO_TOTALS AS\n",
    "SELECT\n",
    "  PurchaseOrderID,\n",
    "  SUM(COALESCE({qty_col},0) * COALESCE({price_col},0)) AS POAmount\n",
    "FROM {PO_SCHEMA}.{PO_TABLE}\n",
    "GROUP BY 1\n",
    "\"\"\")\n",
    "print(\"po_totals sample:\", cs.execute(f\"SELECT * FROM {PO_SCHEMA}.PO_TOTALS LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built PUBLIC.PURCHASE_ORDERS_AND_INVOICES as TABLE.\n",
      "Row count: 2072\n",
      "[(83481, 4, 562, datetime.date(2019, 12, 5), Decimal('402192.00'), Decimal('402192.0000'), Decimal('0.0000')), (85554, 4, 576, datetime.date(2019, 12, 16), Decimal('407994.00'), Decimal('407994.0000'), Decimal('0.0000')), (121887, 7, 814, datetime.date(2020, 5, 7), Decimal('186559.60'), Decimal('186559.6000'), Decimal('0.0000')), (85557, 7, 577, datetime.date(2019, 12, 16), Decimal('98336.40'), Decimal('98336.4000'), Decimal('0.0000')), (86182, 4, 581, datetime.date(2019, 12, 18), Decimal('413298.00'), Decimal('413298.0000'), Decimal('0.0000')), (86385, 4, 583, datetime.date(2019, 12, 19), Decimal('413682.00'), Decimal('413682.0000'), Decimal('0.0000')), (87795, 7, 594, datetime.date(2019, 12, 25), Decimal('102054.80'), Decimal('102054.8000'), Decimal('0.0000')), (123490, 4, 821, datetime.date(2020, 5, 13), Decimal('569754.00'), Decimal('569754.0000'), Decimal('0.0000')), (123675, 4, 823, datetime.date(2020, 5, 14), Decimal('569790.00'), Decimal('569790.0000'), Decimal('0.0000')), (124573, 7, 828, datetime.date(2020, 5, 16), Decimal('189055.20'), Decimal('189055.2000'), Decimal('0.0000'))]\n"
     ]
    }
   ],
   "source": [
    "##Creating the view where we merged Purchase Orders and Invoices are: PURCHASE_ORDERS_AND_INVOICES as TABLE.\n",
    "\n",
    "\n",
    "# Make sure we're in the right place (safe to re-run)\n",
    "cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "\n",
    "# Try MATERIALIZED VIEW first; fall back to TABLE if not supported\n",
    "built = \"MATERIALIZED VIEW\"\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE MATERIALIZED VIEW PUBLIC.PURCHASE_ORDERS_AND_INVOICES AS\n",
    "    SELECT\n",
    "      si.SupplierTransactionID,\n",
    "      si.SupplierID,\n",
    "      si.PurchaseOrderID,\n",
    "      si.TransactionDate                 AS InvoiceDate,\n",
    "      si.AmountExcludingTax,\n",
    "      pt.POAmount,\n",
    "      (si.AmountExcludingTax - pt.POAmount) AS invoiced_vs_quoted\n",
    "    FROM PUBLIC.SUPPLIER_INVOICES si\n",
    "    JOIN PUBLIC.PO_TOTALS pt USING (PurchaseOrderID)\n",
    "    \"\"\")\n",
    "except Exception:\n",
    "    built = \"TABLE\"\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE PUBLIC.PURCHASE_ORDERS_AND_INVOICES AS\n",
    "    SELECT\n",
    "      si.SupplierTransactionID,\n",
    "      si.SupplierID,\n",
    "      si.PurchaseOrderID,\n",
    "      si.TransactionDate                 AS InvoiceDate,\n",
    "      si.AmountExcludingTax,\n",
    "      pt.POAmount,\n",
    "      (si.AmountExcludingTax - pt.POAmount) AS invoiced_vs_quoted\n",
    "    FROM PUBLIC.SUPPLIER_INVOICES si\n",
    "    JOIN PUBLIC.PO_TOTALS pt USING (PurchaseOrderID)\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"Built PUBLIC.PURCHASE_ORDERS_AND_INVOICES as {built}.\")\n",
    "print(\"Row count:\", cs.execute(\"SELECT COUNT(*) FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES LIMIT 10\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing connections to Postgres to get our Vendor Info \n",
    "\n",
    "# If host is macOS/Windows this works out of the box:\n",
    "PG_HOST = \"host.docker.internal\"\n",
    "\n",
    "# If you're on Linux and host.docker.internal doesn't resolve, uncomment this instead:\n",
    "# import subprocess\n",
    "# PG_HOST = subprocess.check_output(\"ip route | awk '/default/ {print $3}'\", shell=True).decode().strip()\n",
    "\n",
    "PG_PORT = 8765            # <-- from your VS Code connection\n",
    "PG_DB   = \"rsm-docker\"    # <-- from your VS Code connection\n",
    "PG_USER = \"jovyan\"        # <-- from your VS Code connection\n",
    "PG_PWD  = \"postgres\"      # <-- from your VS Code connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Postgres at host.docker.internal 8765\n",
      "Tables named supplier_case: [('public', 'supplier_case')]\n",
      "Columns: ['supplierid', 'suppliername', 'suppliercategoryid', 'primarycontactpersonid', 'alternatecontactpersonid', 'deliverymethodid', 'postalcityid', 'supplierreference', 'bankaccountname', 'bankaccountbranch', 'bankaccountcode', 'bankaccountnumber', 'bankinternationalcode', 'paymentdays', 'internalcomments', 'phonenumber', 'faxnumber', 'websiteurl', 'deliveryaddressline1', 'deliveryaddressline2', 'deliverypostalcode', 'deliverylocation', 'postaladdressline1', 'postaladdressline2', 'postalpostalcode', 'lasteditedby', 'validfrom', 'validto']\n",
      "Row count: 13\n"
     ]
    }
   ],
   "source": [
    "##Connecting to postgres, and copying the table to create supplier_Case table (One time)\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=PG_HOST, port=PG_PORT, dbname=PG_DB, user=PG_USER, password=PG_PWD)\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    print(\"✅ Connected to Postgres at\", PG_HOST, PG_PORT)\n",
    "\n",
    "    # Find the table (unquoted identifiers in Postgres become lowercase)\n",
    "    cur.execute(\"\"\"\n",
    "      SELECT table_schema, table_name\n",
    "      FROM information_schema.tables\n",
    "      WHERE table_name ILIKE 'supplier_case'\n",
    "    \"\"\")\n",
    "    print(\"Tables named supplier_case:\", cur.fetchall())\n",
    "\n",
    "    # Show columns (you’ll likely see supplierid, suppliername, postalpostalcode)\n",
    "    cur.execute(\"\"\"\n",
    "      SELECT column_name\n",
    "      FROM information_schema.columns\n",
    "      WHERE table_name ILIKE 'supplier_case'\n",
    "      ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    cols = [r[0] for r in cur.fetchall()]\n",
    "    print(\"Columns:\", cols)\n",
    "\n",
    "    # Quick row count\n",
    "    cur.execute('SELECT COUNT(*) FROM public.supplier_case')\n",
    "    print(\"Row count:\", cur.fetchone()[0])\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"❌ Postgres connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported CSV: /home/jovyan/sf_uploads/supplier_case_export.csv\n"
     ]
    }
   ],
   "source": [
    "##Exporting the previous step as a csv so that we can now refer to it within the repo \n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "\n",
    "conn = psycopg2.connect(host=PG_HOST, port=PG_PORT, dbname=PG_DB, user=PG_USER, password=PG_PWD)\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "safe_dir = Path.home() / \"sf_uploads\"\n",
    "safe_dir.mkdir(parents=True, exist_ok=True)\n",
    "export_csv = safe_dir / \"supplier_case_export.csv\"\n",
    "\n",
    "# Use lowercase column names per Postgres rules\n",
    "with open(export_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    cur.copy_expert(\n",
    "        \"\"\"\n",
    "        COPY (\n",
    "            SELECT supplierid, suppliername, postalpostalcode\n",
    "            FROM public.supplier_case\n",
    "        ) TO STDOUT WITH CSV HEADER\n",
    "        \"\"\",\n",
    "        f\n",
    "    )\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Exported CSV:\", export_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged: [('refdata_stage/supplier_case_export.csv.gz', 320, 'afd5053a5689ecf4cc02688db7c733da', 'Sun, 14 Sep 2025 23:46:14 GMT')]\n",
      "Rows in PUBLIC.SUPPLIER_CASE: 13\n",
      "[('1', 'A Datum Corporation', '22202'), ('2', 'Contoso, Ltd.', '80125'), ('3', 'Consolidated Messenger', '60523'), ('4', 'Fabrikam, Inc.', '95642'), ('5', 'Graphic Design Institute', '80125')]\n"
     ]
    }
   ],
   "source": [
    "##Actually pushing it to snowflake \n",
    "\n",
    "# Use your existing Snowflake cursor `cs`\n",
    "uri = export_csv.resolve().as_uri()\n",
    "\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS PUBLIC.refdata_stage\")\n",
    "cs.execute(f\"PUT '{uri}' @PUBLIC.refdata_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "print(\"Staged:\", cs.execute(\"LIST @PUBLIC.refdata_stage\").fetchall())\n",
    "\n",
    "# Keep STRING types to preserve leading zeros in ZIPs\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.SUPPLIER_CASE (\n",
    "  SupplierID STRING,\n",
    "  SupplierName STRING,\n",
    "  PostalPostalCode STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PUBLIC.SUPPLIER_CASE\n",
    "FROM (SELECT $1, $2, $3 FROM @PUBLIC.refdata_stage)\n",
    "FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1)\n",
    "ON_ERROR=ABORT_STATEMENT\n",
    "\"\"\")\n",
    "\n",
    "print(\"Rows in PUBLIC.SUPPLIER_CASE:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_CASE\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.SUPPLIER_CASE LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2438\n",
      "2074\n",
      "2072\n",
      "13\n",
      "[(83481, 4, 562, datetime.date(2019, 12, 5), Decimal('402192.00'), Decimal('402192.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (85554, 4, 576, datetime.date(2019, 12, 16), Decimal('407994.00'), Decimal('407994.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (121887, 7, 814, datetime.date(2020, 5, 7), Decimal('186559.60'), Decimal('186559.6000'), Decimal('0.0000'), 'Litware, Inc.', '95642'), (85557, 7, 577, datetime.date(2019, 12, 16), Decimal('98336.40'), Decimal('98336.4000'), Decimal('0.0000'), 'Litware, Inc.', '95642'), (86182, 4, 581, datetime.date(2019, 12, 18), Decimal('413298.00'), Decimal('413298.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (86385, 4, 583, datetime.date(2019, 12, 19), Decimal('413682.00'), Decimal('413682.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (87795, 7, 594, datetime.date(2019, 12, 25), Decimal('102054.80'), Decimal('102054.8000'), Decimal('0.0000'), 'Litware, Inc.', '95642'), (123490, 4, 821, datetime.date(2020, 5, 13), Decimal('569754.00'), Decimal('569754.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (123675, 4, 823, datetime.date(2020, 5, 14), Decimal('569790.00'), Decimal('569790.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (124573, 7, 828, datetime.date(2020, 5, 16), Decimal('189055.20'), Decimal('189055.2000'), Decimal('0.0000'), 'Litware, Inc.', '95642')]\n"
     ]
    }
   ],
   "source": [
    "##Intermittent check step to make sure that all relevant tables are pulling correctly \n",
    "\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.PO_TOTALS\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_CASE\").fetchone()[0])\n",
    "\n",
    "# Join suppliers onto the PO/invoice view (zip normalized)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PUBLIC.JOINED_WITH_SUPPLIERS AS\n",
    "SELECT\n",
    "  p.*,\n",
    "  s.SupplierName,\n",
    "  LPAD(REGEXP_REPLACE(s.PostalPostalCode,'\\\\D',''),5,'0') AS PostalPostalCode\n",
    "FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES p\n",
    "LEFT JOIN PUBLIC.SUPPLIER_CASE s\n",
    "  ON TO_VARCHAR(p.SupplierID) = TO_VARCHAR(s.SupplierID)\n",
    "\"\"\")\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.JOINED_WITH_SUPPLIERS LIMIT 10\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZCTA extracted: /home/jovyan/sf_uploads/2021_Gaz_zcta_national.txt\n",
      "Staged: [('zcta_stage/2021_Gaz_zcta_national.txt.gz', 966848, 'c040089a8850ac7d8e74238beacb9315', 'Mon, 15 Sep 2025 00:09:49 GMT')]\n",
      "ZCTA rows: 33791\n",
      "[('00601', 18.180555, -66.749961), ('00602', 18.361945, -67.175597), ('00603', 18.458497, -67.123906), ('00606', 18.158327, -66.932928), ('00610', 18.294032, -67.127156)]\n"
     ]
    }
   ],
   "source": [
    "##Getting the US Census Data from the zip file within the repo \n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# Unzip the file\n",
    "zcta_zip = Path(\"Data\") / \"2021_Gaz_zcta_national.zip\"\n",
    "safe_dir = Path.home() / \"sf_uploads\"\n",
    "safe_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zcta_zip, 'r') as zf:\n",
    "    member = [m for m in zf.namelist() if m.lower().endswith(\".txt\")][0]\n",
    "    zf.extract(member, safe_dir)\n",
    "    zcta_txt = safe_dir / member\n",
    "\n",
    "print(\"ZCTA extracted:\", zcta_txt)\n",
    "\n",
    "# Stage the file\n",
    "uri = zcta_txt.resolve().as_uri()\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS PUBLIC.zcta_stage\")\n",
    "cs.execute(f\"PUT '{uri}' @PUBLIC.zcta_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "print(\"Staged:\", cs.execute(\"LIST @PUBLIC.zcta_stage\").fetchall())\n",
    "\n",
    "# Create table for ZIP, lat, lon\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.ZCTA_2021 (\n",
    "  GEOID     STRING,\n",
    "  INTPTLAT  FLOAT,\n",
    "  INTPTLONG FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Define TSV file format (skip header, tab delimiter)\n",
    "# File format: tab-delimited, header row, values optionally quoted, trim spaces\n",
    "# Use the TSV format we already created\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT PUBLIC.TSV_FF\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = '\\t'\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  TRIM_SPACE = TRUE\n",
    "  SKIP_HEADER = 1\n",
    "  NULL_IF = ('','NULL')\n",
    "\"\"\")\n",
    "\n",
    "# Recreate the target table\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.ZCTA_2021 (\n",
    "  GEOID     STRING,\n",
    "  INTPTLAT  DOUBLE,\n",
    "  INTPTLONG DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# COPY using correct column positions: $1 (GEOID), $6 (LAT), $7 (LON)\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PUBLIC.ZCTA_2021 (GEOID, INTPTLAT, INTPTLONG)\n",
    "FROM (\n",
    "  SELECT\n",
    "    $1,\n",
    "    TRY_TO_DOUBLE(REPLACE($6,'+','')),\n",
    "    TRY_TO_DOUBLE(REPLACE($7,'+',''))\n",
    "  FROM @PUBLIC.zcta_stage (FILE_FORMAT => 'PUBLIC.TSV_FF')\n",
    ")\n",
    "ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")\n",
    "\n",
    "print(\"ZCTA rows:\", cs.execute(\"SELECT COUNT(*) FROM PUBLIC.ZCTA_2021\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT GEOID, INTPTLAT, INTPTLONG FROM PUBLIC.ZCTA_2021 ORDER BY GEOID LIMIT 5\").fetchall())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate clean table\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.ZCTA_2021 (\n",
    "  GEOID     STRING,\n",
    "  INTPTLAT  DOUBLE,\n",
    "  INTPTLONG DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# COPY: $1=GEOID, $8=INTPTLAT, $9=INTPTLONG; remove leading '+' before casting\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PUBLIC.ZCTA_2021 (GEOID, INTPTLAT, INTPTLONG)\n",
    "FROM (\n",
    "  SELECT\n",
    "    $1,\n",
    "    TRY_TO_DOUBLE(REPLACE($8,'+','')),\n",
    "    TRY_TO_DOUBLE(REPLACE($9,'+',''))\n",
    "  FROM @PUBLIC.zcta_stage (FILE_FORMAT => 'PUBLIC.TSV_FF')\n",
    ")\n",
    "ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")\n",
    "\n",
    "print(\"ZCTA rows:\", cs.execute(\"SELECT COUNT(*) FROM PUBLIC.ZCTA_2021\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT GEOID, INTPTLAT, INTPTLONG FROM PUBLIC.ZCTA_2021 ORDER BY GEOID LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplier ZIPs with geo: 8\n"
     ]
    }
   ],
   "source": [
    "# Unique supplier ZIPs (normalize to 5 digits)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW PUBLIC.SUPPLIER_ZIPS AS\n",
    "SELECT DISTINCT LPAD(REGEXP_REPLACE(PostalPostalCode,'\\\\D',''),5,'0') AS ZIP5\n",
    "FROM PUBLIC.SUPPLIER_CASE\n",
    "WHERE PostalPostalCode IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Join ZIPs to ZCTA lat/lon\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW PUBLIC.SUPPLIER_ZIPS_WITH_GEO AS\n",
    "SELECT sz.ZIP5, z.INTPTLAT AS LAT, z.INTPTLONG AS LON\n",
    "FROM PUBLIC.SUPPLIER_ZIPS sz\n",
    "JOIN PUBLIC.ZCTA_2021 z\n",
    "  ON z.GEOID = sz.ZIP5\n",
    "\"\"\")\n",
    "print(\"Supplier ZIPs with geo:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_ZIPS_WITH_GEO\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station index columns: ['COUNTRY_GEO_ID', 'WEATHER_STATION_NETWORK', 'STATE_GEO_ID', 'LONGITUDE', 'ASSOCIATED_NETWORKS', 'ZIP_NAME', 'WORLD_METEOROLOGICAL_ORGANIZATION_ID', 'LATITUDE', 'ZIP_GEO_ID', 'COUNTRY_NAME', 'STATE_NAME', 'NOAA_WEATHER_STATION_NAME', 'NOAA_WEATHER_STATION_ID', 'ELEVATION', 'SOURCE_DATA']\n",
      "Metrics columns      : ['UNIT', 'VARIABLE', 'DATETIME', 'VALUE', 'NOAA_WEATHER_STATION_ID', 'VARIABLE_NAME', 'DATE']\n",
      "ZIP→station rows: 8\n",
      "[('06331', 'US1CTWN0011', 8.101088905729062), ('34269', 'US1FLCH0054', 8.065228870939682), ('95642', 'US1CAAM0011', 0.6813252694400262), ('42437', 'USC00155569', 8.955611989180575), ('80125', 'US1CODG0159', 1.7581248900134214)]\n"
     ]
    }
   ],
   "source": [
    "# --- Find weather DB/schema (reuse your function if already defined) ---\n",
    "def find_weather_db(cs):\n",
    "    names = [r[1] for r in cs.execute(\"SHOW DATABASES\").fetchall()]\n",
    "    for cand in (\"WEATHER__ENVIRONMENT\",\"WEATHER_ENVIRONMENT\"):\n",
    "        if cand in names: return cand\n",
    "    for n in names:\n",
    "        if \"WEATHER\" in n and \"ENVIRONMENT\" in n: return n\n",
    "    return None\n",
    "\n",
    "db = find_weather_db(cs)\n",
    "schemas = {r[1] for r in cs.execute(f\"SHOW SCHEMAS IN DATABASE {db}\").fetchall()}\n",
    "wx_schema = \"CYBERSYN\" if \"CYBERSYN\" in schemas else \"PUBLIC\"\n",
    "\n",
    "station_idx = f\"{db}.{wx_schema}.NOAA_WEATHER_STATION_INDEX\"\n",
    "metrics_ts  = f\"{db}.{wx_schema}.NOAA_WEATHER_METRICS_TIMESERIES\"\n",
    "\n",
    "# --- Columns present in each table ---\n",
    "st_cols = [r[0] for r in cs.execute(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM {db}.INFORMATION_SCHEMA.COLUMNS\n",
    "  WHERE table_schema='{wx_schema}' AND table_name='NOAA_WEATHER_STATION_INDEX'\n",
    "\"\"\").fetchall()]\n",
    "mt_cols = [r[0] for r in cs.execute(f\"\"\"\n",
    "  SELECT column_name\n",
    "  FROM {db}.INFORMATION_SCHEMA.COLUMNS\n",
    "  WHERE table_schema='{wx_schema}' AND table_name='NOAA_WEATHER_METRICS_TIMESERIES'\n",
    "\"\"\").fetchall()]\n",
    "STU, MTU = [c.upper() for c in st_cols], [c.upper() for c in mt_cols]\n",
    "print(\"Station index columns:\", st_cols)\n",
    "print(\"Metrics columns      :\", mt_cols)\n",
    "\n",
    "# Hard-set from your printed columns\n",
    "lat_col      = \"LATITUDE\"\n",
    "lon_col      = \"LONGITUDE\"\n",
    "stn_idx_col  = \"NOAA_WEATHER_STATION_ID\"  # in station index\n",
    "stn_met_col  = \"NOAA_WEATHER_STATION_ID\"  # in metrics\n",
    "\n",
    "# Build ZIP -> nearest station (CROSS JOIN + Haversine)\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW PUBLIC.ZIP_TO_NEAREST_STATION AS\n",
    "WITH pairs AS (\n",
    "  SELECT\n",
    "    z.ZIP5,\n",
    "    s.{stn_idx_col} AS STATION_KEY,\n",
    "    s.{lat_col}     AS s_lat,\n",
    "    s.{lon_col}     AS s_lon,\n",
    "    z.LAT           AS z_lat,\n",
    "    z.LON           AS z_lon,\n",
    "    2*6371*ASIN(SQRT(\n",
    "        POWER(SIN(RADIANS(z.LAT - s.{lat_col})/2),2)\n",
    "      + COS(RADIANS(z.LAT))*COS(RADIANS(s.{lat_col}))\n",
    "      * POWER(SIN(RADIANS(z.LON - s.{lon_col})/2),2)\n",
    "    )) AS km_distance\n",
    "  FROM PUBLIC.SUPPLIER_ZIPS_WITH_GEO z\n",
    "  CROSS JOIN {station_idx} s\n",
    ")\n",
    "SELECT ZIP5, STATION_KEY, km_distance\n",
    "FROM (\n",
    "  SELECT p.*, ROW_NUMBER() OVER (PARTITION BY ZIP5 ORDER BY km_distance) AS rn\n",
    "  FROM pairs p\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"ZIP→station rows:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.ZIP_TO_NEAREST_STATION\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.ZIP_TO_NEAREST_STATION LIMIT 5\").fetchall())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built PUBLIC.SUPPLIER_ZIP_CODE_WEATHER as VIEW.\n",
      "[('42437', datetime.date(2007, 10, 24), 17.2), ('42437', datetime.date(2006, 4, 24), 23.9), ('42437', datetime.date(2006, 6, 28), 26.7), ('42437', datetime.date(2008, 11, 27), 14.4), ('42437', datetime.date(2008, 6, 4), 32.2), ('42437', datetime.date(2006, 4, 23), 27.8), ('42437', datetime.date(2007, 10, 27), 13.3), ('42437', datetime.date(2008, 2, 28), 0.0), ('42437', datetime.date(2007, 3, 16), 10.0), ('42437', datetime.date(2005, 11, 26), 6.7)]\n",
      "Distinct zips covered: 1\n"
     ]
    }
   ],
   "source": [
    "# Compose table names\n",
    "metrics_ts = f\"{db}.{wx_schema}.NOAA_WEATHER_METRICS_TIMESERIES\"\n",
    "\n",
    "# Build as MV if allowed; otherwise VIEW\n",
    "ddl = f\"\"\"\n",
    "SELECT\n",
    "  z.ZIP5                  AS zip_code,\n",
    "  m.DATE                  AS date,             -- daily grain\n",
    "  CAST(m.VALUE AS FLOAT)  AS high_temperature  -- VALUE already numeric\n",
    "FROM PUBLIC.ZIP_TO_NEAREST_STATION z\n",
    "JOIN {metrics_ts} m\n",
    "  ON m.{stn_met_col} = z.STATION_KEY\n",
    "WHERE m.VALUE IS NOT NULL\n",
    "  AND (\n",
    "        UPPER(m.VARIABLE) = 'TMAX'\n",
    "     OR (UPPER(m.VARIABLE_NAME) LIKE '%MAX%' AND UPPER(m.VARIABLE_NAME) LIKE '%TEMP%')\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "built = \"MATERIALIZED VIEW\"\n",
    "try:\n",
    "    cs.execute(f\"CREATE OR REPLACE MATERIALIZED VIEW PUBLIC.SUPPLIER_ZIP_CODE_WEATHER AS {ddl}\")\n",
    "except Exception:\n",
    "    built = \"VIEW\"\n",
    "    cs.execute(f\"CREATE OR REPLACE VIEW PUBLIC.SUPPLIER_ZIP_CODE_WEATHER AS {ddl}\")\n",
    "\n",
    "print(f\"Built PUBLIC.SUPPLIER_ZIP_CODE_WEATHER as {built}.\")\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.SUPPLIER_ZIP_CODE_WEATHER LIMIT 10\").fetchall())\n",
    "print(\"Distinct zips covered:\",\n",
    "      cs.execute(\"SELECT COUNT(DISTINCT zip_code) FROM PUBLIC.SUPPLIER_ZIP_CODE_WEATHER\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP×date pairs: 914\n"
     ]
    }
   ],
   "source": [
    "# Distinct supplier ZIPs × invoice dates you actually care about\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW PUBLIC.NEEDED_ZIP_DATES AS\n",
    "SELECT\n",
    "  LPAD(REGEXP_REPLACE(s.PostalPostalCode,'\\\\D',''),5,'0') AS ZIP5,\n",
    "  p.InvoiceDate                                           AS DATE\n",
    "FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES p\n",
    "JOIN PUBLIC.SUPPLIER_CASE s\n",
    "  ON TO_VARCHAR(p.SupplierID) = TO_VARCHAR(s.SupplierID)\n",
    "GROUP BY 1,2\n",
    "\"\"\")\n",
    "print(\"ZIP×date pairs:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.NEEDED_ZIP_DATES\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From your schema: station index & metrics columns\n",
    "db_schemas = [r[1] for r in cs.execute(\"SHOW DATABASES\").fetchall()]\n",
    "def find_weather_db():\n",
    "    for cand in (\"WEATHER__ENVIRONMENT\",\"WEATHER_ENVIRONMENT\"):\n",
    "        if cand in db_schemas: return cand\n",
    "    for n in db_schemas:\n",
    "        if \"WEATHER\" in n and \"ENVIRONMENT\" in n: return n\n",
    "    return None\n",
    "\n",
    "db = find_weather_db()\n",
    "schemas = {r[1] for r in cs.execute(f\"SHOW SCHEMAS IN DATABASE {db}\").fetchall()}\n",
    "wx_schema = \"CYBERSYN\" if \"CYBERSYN\" in schemas else \"PUBLIC\"\n",
    "\n",
    "station_idx = f\"{db}.{wx_schema}.NOAA_WEATHER_STATION_INDEX\"\n",
    "metrics_ts  = f\"{db}.{wx_schema}.NOAA_WEATHER_METRICS_TIMESERIES\"\n",
    "\n",
    "lat_col     = \"LATITUDE\"\n",
    "lon_col     = \"LONGITUDE\"\n",
    "stn_idx_col = \"NOAA_WEATHER_STATION_ID\"\n",
    "stn_met_col = \"NOAA_WEATHER_STATION_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built PUBLIC.SUPPLIER_ZIP_CODE_WEATHER as VIEW.\n",
      "[('95642', datetime.date(2022, 5, 20), 29.4), ('95642', datetime.date(2019, 7, 24), 36.1), ('95642', datetime.date(2020, 3, 27), 11.1), ('95642', datetime.date(2021, 2, 25), 21.7), ('95642', datetime.date(2019, 6, 11), 35.0), ('95642', datetime.date(2020, 6, 24), 36.1), ('95642', datetime.date(2021, 5, 18), 24.4), ('95642', datetime.date(2021, 9, 24), 33.9), ('95642', datetime.date(2020, 8, 14), 38.9), ('95642', datetime.date(2022, 1, 12), 18.9)]\n",
      "Distinct zips covered: 3\n",
      "Dates covered (min,max): (datetime.date(2019, 1, 2), datetime.date(2022, 5, 31))\n"
     ]
    }
   ],
   "source": [
    "# We'll filter metrics to \"daily high\" rows and the needed dates only\n",
    "# Then pick nearest station among those with data for that (ZIP, DATE)\n",
    "ddl = f\"\"\"\n",
    "WITH\n",
    "needed AS (\n",
    "  SELECT ZIP5, DATE FROM PUBLIC.NEEDED_ZIP_DATES\n",
    "),\n",
    "daily_max AS (\n",
    "  SELECT\n",
    "    m.{stn_met_col}   AS STATION_KEY,\n",
    "    m.DATE            AS DATE,\n",
    "    CAST(m.VALUE AS FLOAT) AS HIGH_TEMPERATURE\n",
    "  FROM {metrics_ts} m\n",
    "  WHERE m.VALUE IS NOT NULL\n",
    "    AND (\n",
    "         UPPER(m.VARIABLE) = 'TMAX'\n",
    "      OR (UPPER(m.VARIABLE_NAME) LIKE '%MAX%' AND UPPER(m.VARIABLE_NAME) LIKE '%TEMP%')\n",
    "    )\n",
    "),\n",
    "pairs AS (\n",
    "  SELECT\n",
    "    n.ZIP5,\n",
    "    n.DATE,\n",
    "    d.HIGH_TEMPERATURE,\n",
    "    s.{stn_idx_col} AS STATION_KEY,\n",
    "    2*6371*ASIN(SQRT(\n",
    "        POWER(SIN(RADIANS(z.INTPTLAT - s.{lat_col})/2),2)\n",
    "      + COS(RADIANS(z.INTPTLAT))*COS(RADIANS(s.{lat_col}))\n",
    "      * POWER(SIN(RADIANS(z.INTPTLONG - s.{lon_col})/2),2)\n",
    "    )) AS km_distance\n",
    "  FROM needed n\n",
    "  JOIN PUBLIC.ZCTA_2021 z\n",
    "    ON z.GEOID = n.ZIP5\n",
    "  JOIN daily_max d\n",
    "    ON d.DATE = n.DATE\n",
    "  JOIN {station_idx} s\n",
    "    ON s.{stn_idx_col} = d.STATION_KEY\n",
    ")\n",
    "SELECT ZIP5 AS zip_code, DATE, HIGH_TEMPERATURE\n",
    "FROM (\n",
    "  SELECT\n",
    "    p.*,\n",
    "    ROW_NUMBER() OVER (PARTITION BY ZIP5, DATE ORDER BY km_distance) AS rn\n",
    "  FROM pairs p\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "\n",
    "# Create the final per-zip per-day weather view\n",
    "try:\n",
    "    cs.execute(f\"CREATE OR REPLACE MATERIALIZED VIEW PUBLIC.SUPPLIER_ZIP_CODE_WEATHER AS {ddl}\")\n",
    "    built = \"MATERIALIZED VIEW\"\n",
    "except Exception:\n",
    "    cs.execute(f\"CREATE OR REPLACE VIEW PUBLIC.SUPPLIER_ZIP_CODE_WEATHER AS {ddl}\")\n",
    "    built = \"VIEW\"\n",
    "\n",
    "print(f\"Built PUBLIC.SUPPLIER_ZIP_CODE_WEATHER as {built}.\")\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.SUPPLIER_ZIP_CODE_WEATHER LIMIT 10\").fetchall())\n",
    "print(\"Distinct zips covered:\",\n",
    "      cs.execute(\"SELECT COUNT(DISTINCT zip_code) FROM PUBLIC.SUPPLIER_ZIP_CODE_WEATHER\").fetchone()[0])\n",
    "print(\"Dates covered (min,max):\",\n",
    "      cs.execute(\"SELECT MIN(date), MAX(date) FROM PUBLIC.SUPPLIER_ZIP_CODE_WEATHER\").fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rows: 2072\n",
      "Rows with weather: 2069\n",
      "[(4, 2, datetime.date(2019, 1, 2), '95642', 12.2), (7, 4, datetime.date(2019, 1, 2), '95642', 12.2), (10, 5, datetime.date(2019, 1, 2), '22202', 8.9), (5, 3, datetime.date(2019, 1, 2), '80125', -7.2), (12, 6, datetime.date(2019, 1, 2), '80125', -7.2), (2, 1, datetime.date(2019, 1, 2), '80125', -7.2), (7, 9, datetime.date(2019, 1, 3), '95642', 11.1), (4, 7, datetime.date(2019, 1, 3), '95642', 11.1), (12, 11, datetime.date(2019, 1, 3), '80125', 3.9), (5, 8, datetime.date(2019, 1, 3), '80125', 3.9)]\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PUBLIC.FINAL_SUPPLIER_PO_INVOICE_WEATHER AS\n",
    "SELECT\n",
    "  p.*,\n",
    "  s.SupplierName,\n",
    "  LPAD(REGEXP_REPLACE(s.PostalPostalCode,'\\\\D',''),5,'0') AS SupplierZIP,\n",
    "  w.HIGH_TEMPERATURE\n",
    "FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES p\n",
    "LEFT JOIN PUBLIC.SUPPLIER_CASE s\n",
    "  ON TO_VARCHAR(p.SupplierID) = TO_VARCHAR(s.SupplierID)\n",
    "LEFT JOIN PUBLIC.SUPPLIER_ZIP_CODE_WEATHER w\n",
    "  ON w.zip_code = LPAD(REGEXP_REPLACE(s.PostalPostalCode,'\\\\D',''),5,'0')\n",
    " AND w.date = p.InvoiceDate\n",
    "\"\"\")\n",
    "\n",
    "print(\"Final rows:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.FINAL_SUPPLIER_PO_INVOICE_WEATHER\").fetchone()[0])\n",
    "print(\"Rows with weather:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.FINAL_SUPPLIER_PO_INVOICE_WEATHER WHERE HIGH_TEMPERATURE IS NOT NULL\").fetchone()[0])\n",
    "\n",
    "print(cs.execute(\"\"\"\n",
    "  SELECT SupplierID, PurchaseOrderID, InvoiceDate, SupplierZIP, HIGH_TEMPERATURE\n",
    "  FROM PUBLIC.FINAL_SUPPLIER_PO_INVOICE_WEATHER\n",
    "  WHERE HIGH_TEMPERATURE IS NOT NULL\n",
    "  ORDER BY InvoiceDate\n",
    "  LIMIT 10\n",
    "\"\"\").fetchall())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

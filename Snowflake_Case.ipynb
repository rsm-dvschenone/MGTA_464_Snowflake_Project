{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting snowflake-connector-python<4,>=3\n",
      "  Using cached snowflake_connector_python-3.17.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4,>=3)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python<4,>=3)\n",
      "  Using cached boto3-1.40.30-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (1.35.90)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (43.0.3)\n",
      "Collecting pyOpenSSL<26.0.0,>=22.0.0 (from snowflake-connector-python<4,>=3)\n",
      "  Downloading pyopenssl-25.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2.9.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2.32.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (24.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (4.12.2)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (3.16.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (4.3.6)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.12/site-packages (from snowflake-connector-python<4,>=3) (0.13.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4,>=3) (2.22)\n",
      "Collecting cryptography>=3.1.0 (from snowflake-connector-python<4,>=3)\n",
      "  Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0->snowflake-connector-python<4,>=3) (2.2.3)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python<4,>=3)\n",
      "  Using cached botocore-1.40.30-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python<4,>=3) (1.0.1)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.24->snowflake-connector-python<4,>=3)\n",
      "  Using cached s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python<4,>=3) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python<4,>=3) (1.16.0)\n",
      "Using cached snowflake_connector_python-3.17.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading pyopenssl-25.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m784.9 kB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached boto3-1.40.30-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.40.30-py3-none-any.whl (14.0 MB)\n",
      "Using cached s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: asn1crypto, cryptography, botocore, s3transfer, pyOpenSSL, boto3, snowflake-connector-python\n",
      "\u001b[2K  Attempting uninstall: cryptography\n",
      "\u001b[2K    Found existing installation: cryptography 43.0.3\n",
      "\u001b[2K    Uninstalling cryptography-43.0.3:\n",
      "\u001b[2K      Successfully uninstalled cryptography-43.0.3━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K  Attempting uninstall: botocore━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K    Found existing installation: botocore 1.35.90━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K    Uninstalling botocore-1.35.90:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [cryptography]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.35.90━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [botocore]\n",
      "\u001b[2K  Attempting uninstall: s3transfer[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [botocore]\n",
      "\u001b[2K    Found existing installation: s3transfer 0.10.4━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [botocore]\n",
      "\u001b[2K    Uninstalling s3transfer-0.10.4:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [s3transfer]\n",
      "\u001b[2K      Successfully uninstalled s3transfer-0.10.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [s3transfer]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [snowflake-connector-python]-connector-python]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.36.31 requires botocore==1.35.90, but you have botocore 1.40.30 which is incompatible.\n",
      "awscli 1.36.31 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asn1crypto-1.5.1 boto3-1.40.30 botocore-1.40.30 cryptography-45.0.7 pyOpenSSL-25.2.0 s3transfer-0.14.0 snowflake-connector-python-3.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install \"snowflake-connector-python>=3,<4\" python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# Find and load the nearest .env (project root)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "def need(name):\n",
    "    v = os.getenv(name)\n",
    "    if not v:\n",
    "        raise RuntimeError(f\"Missing required env var: {name}\")\n",
    "    # remove accidental quotes/spaces\n",
    "    return v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "SNOWFLAKE_USER     = need(\"SNOWFLAKE_USER\")\n",
    "SNOWFLAKE_PASSWORD = need(\"SNOWFLAKE_PASSWORD\")\n",
    "SNOWFLAKE_ACCOUNT  = need(\"SNOWFLAKE_ACCOUNT\")  # e.g., afaypkh-rjb48354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd1002fa70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS my_first_warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd1002fa70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS testdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 13\n",
      "\n",
      "Sample rows:\n",
      "(1, 'A Datum Corporation', '(847) 555-0100', 'http://www.adatum.com', None, None)\n",
      "(2, 'Contoso, Ltd.', '(360) 555-0100', 'http://www.contoso.com', None, None)\n",
      "(3, 'Consolidated Messenger', '(415) 555-0100', 'http://www.consolidatedmessenger.com', None, None)\n",
      "(4, 'Fabrikam, Inc.', '(203) 555-0104', 'http://www.fabrikam.com', None, None)\n",
      "(5, 'Graphic Design Institute', '(406) 555-0105', 'http://www.graphicdesigninstitute.com', None, None)\n",
      "(6, 'Humongous Insurance', '(423) 555-0105', 'http://www.humongousinsurance.com', None, None)\n",
      "(7, 'Litware, Inc.', '(209) 555-0108', 'http://www.litwareinc.com', None, None)\n",
      "(8, 'Lucerne Publishing', '(423) 555-0103', 'http://www.lucernepublishing.com', None, None)\n",
      "(9, 'Nod Publishers', '(252) 555-0100', 'http://www.nodpublishers.com', None, None)\n",
      "(10, 'Northwind Electric Cars', '(201) 555-0105', 'http://www.northwindelectriccars.com', None, None)\n",
      "\n",
      "Schema:\n",
      "SUPPLIERID NUMBER(38,0)\n",
      "SUPPLIERNAME VARCHAR(16777216)\n",
      "SUPPLIERCATEGORYID NUMBER(38,0)\n",
      "PRIMARYCONTACTPERSONID NUMBER(38,0)\n",
      "ALTERNATECONTACTPERSONID NUMBER(38,0)\n",
      "DELIVERYMETHODID NUMBER(38,0)\n",
      "POSTALCITYID NUMBER(38,0)\n",
      "SUPPLIERREFERENCE VARCHAR(16777216)\n",
      "BANKACCOUNTNAME VARCHAR(16777216)\n",
      "BANKACCOUNTBRANCH VARCHAR(16777216)\n",
      "BANKACCOUNTCODE NUMBER(38,0)\n",
      "BANKACCOUNTNUMBER NUMBER(38,0)\n",
      "BANKINTERNATIONALCODE NUMBER(38,0)\n",
      "PAYMENTDAYS NUMBER(38,0)\n",
      "INTERNALCOMMENTS VARCHAR(16777216)\n",
      "PHONENUMBER VARCHAR(16777216)\n",
      "FAXNUMBER VARCHAR(16777216)\n",
      "WEBSITEURL VARCHAR(16777216)\n",
      "DELIVERYADDRESSLINE1 VARCHAR(16777216)\n",
      "DELIVERYADDRESSLINE2 VARCHAR(16777216)\n",
      "DELIVERYPOSTALCODE NUMBER(38,0)\n",
      "DELIVERYLOCATION VARCHAR(16777216)\n",
      "POSTALADDRESSLINE1 VARCHAR(16777216)\n",
      "POSTALADDRESSLINE2 VARCHAR(16777216)\n",
      "POSTALPOSTALCODE NUMBER(38,0)\n",
      "LASTEDITEDBY NUMBER(38,0)\n",
      "VALIDFROM VARCHAR(16777216)\n",
      "VALIDTO VARCHAR(16777216)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ---- 0) connect  ----\n",
    "conn = snowflake.connector.connect(\n",
    "    host=\"qbmhuza-bnb86629.snowflakecomputing.com\",   \n",
    "    account=\"qbmhuza-bnb86629\",                        \n",
    "    user=\"second2\",\n",
    "    password=\"gyczeg6kaHqywownor\",\n",
    "    warehouse=\"COMPUTE_WH\",                           \n",
    ")\n",
    "cs = conn.cursor()\n",
    "\n",
    "# make sure the warehouse will wake itself\n",
    "cs.execute(\"ALTER WAREHOUSE COMPUTE_WH SET AUTO_SUSPEND=60 AUTO_RESUME=TRUE\")\n",
    "\n",
    "# ---- 1) set DB/Schema (use your existing TESTDB) ----\n",
    "DB, SCHEMA = \"TESTDB\", \"PUBLIC\"\n",
    "cs.execute(f\"CREATE DATABASE IF NOT EXISTS {DB}\")\n",
    "cs.execute(f\"CREATE SCHEMA   IF NOT EXISTS {DB}.{SCHEMA}\")\n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "# ---- 2) read and run your .pgsql file from the repo ----\n",
    "sql_path = Path(\"Data/supplier_case.pgsql\")     \n",
    "assert sql_path.exists(), f\"Not found: {sql_path.resolve()}\"\n",
    "txt = sql_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# tiny Postgres -> Snowflake cleanups\n",
    "txt = \"\\n\".join(l for l in txt.splitlines() if not l.strip().startswith(\"\\\\\"))            # drop psql meta commands\n",
    "txt = re.sub(r\"\\bNUMERIC\\b\", \"NUMBER\", txt, flags=re.I)                                   # NUMERIC -> NUMBER (safe)\n",
    "txt = re.sub(r\"\\bsupplier_case\\b\", f\"{DB}.{SCHEMA}.SUPPLIER_CASE\", txt, flags=re.I)       # fully-qualify table\n",
    "\n",
    "# split on semicolons and execute\n",
    "stmts = [s.strip() for s in re.split(r\";\\s*(?=\\n|$)\", txt) if s.strip()]\n",
    "for s in stmts:\n",
    "    cs.execute(s)\n",
    "\n",
    "# ---- 3) visualize (still only cs.execute) ----\n",
    "print(\"Rows:\", cs.execute(f\"SELECT COUNT(*) FROM {DB}.{SCHEMA}.SUPPLIER_CASE\").fetchone()[0])\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "for r in cs.execute(f\"\"\"\n",
    "    SELECT SupplierID, SupplierName, PhoneNumber, WebsiteURL,\n",
    "           TRY_TO_DATE(ValidFrom) AS ValidFrom, TRY_TO_DATE(ValidTo) AS ValidTo\n",
    "    FROM {DB}.{SCHEMA}.SUPPLIER_CASE\n",
    "    ORDER BY SupplierID\n",
    "    LIMIT 10\n",
    "\"\"\").fetchall():\n",
    "    print(r)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "for r in cs.execute(f\"DESCRIBE TABLE {DB}.{SCHEMA}.SUPPLIER_CASE\").fetchall():\n",
    "    print(r[0], r[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd1002ff80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE DATABASE TESTDB\")\n",
    "cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN AS\n",
    "SELECT\n",
    "  CAST(SUPPLIERID               AS INT)        AS SUPPLIERID,\n",
    "  SUPPLIERNAME                                   AS SUPPLIERNAME,\n",
    "  CAST(SUPPLIERCATEGORYID       AS INT)        AS SUPPLIERCATEGORYID,\n",
    "  CAST(PRIMARYCONTACTPERSONID   AS INT)        AS PRIMARYCONTACTPERSONID,\n",
    "  CAST(ALTERNATECONTACTPERSONID AS INT)        AS ALTERNATECONTACTPERSONID,\n",
    "  CAST(DELIVERYMETHODID         AS INT)        AS DELIVERYMETHODID,\n",
    "  CAST(POSTALCITYID             AS INT)        AS POSTALCITYID,\n",
    "  SUPPLIERREFERENCE                              AS SUPPLIERREFERENCE,\n",
    "  PHONENUMBER                                   AS PHONENUMBER,\n",
    "  WEBSITEURL                                    AS WEBSITEURL,\n",
    "  DELIVERYADDRESSLINE1                           AS DELIVERYADDRESSLINE1,\n",
    "  CAST(DELIVERYPOSTALCODE       AS INT)        AS DELIVERYPOSTALCODE,\n",
    "  POSTALADDRESSLINE1                             AS POSTALADDRESSLINE1,\n",
    "  CAST(POSTALPOSTALCODE         AS INT)        AS POSTALPOSTALCODE,\n",
    "  CAST(LASTEDITEDBY             AS INT)        AS LASTEDITEDBY,\n",
    "  TRY_TO_DATE(VALIDFROM)                        AS VALIDFROM,\n",
    "  TRY_TO_DATE(VALIDTO)                          AS VALIDTO\n",
    "FROM TESTDB.PUBLIC.SUPPLIER_CASE;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "(1, 'A Datum Corporation', '(847) 555-0100', 'http://www.adatum.com', None, None)\n",
      "(2, 'Contoso, Ltd.', '(360) 555-0100', 'http://www.contoso.com', None, None)\n",
      "(3, 'Consolidated Messenger', '(415) 555-0100', 'http://www.consolidatedmessenger.com', None, None)\n",
      "(4, 'Fabrikam, Inc.', '(203) 555-0104', 'http://www.fabrikam.com', None, None)\n",
      "(5, 'Graphic Design Institute', '(406) 555-0105', 'http://www.graphicdesigninstitute.com', None, None)\n",
      "(6, 'Humongous Insurance', '(423) 555-0105', 'http://www.humongousinsurance.com', None, None)\n",
      "(7, 'Litware, Inc.', '(209) 555-0108', 'http://www.litwareinc.com', None, None)\n",
      "(8, 'Lucerne Publishing', '(423) 555-0103', 'http://www.lucernepublishing.com', None, None)\n",
      "(9, 'Nod Publishers', '(252) 555-0100', 'http://www.nodpublishers.com', None, None)\n",
      "(10, 'Northwind Electric Cars', '(201) 555-0105', 'http://www.northwindelectriccars.com', None, None)\n"
     ]
    }
   ],
   "source": [
    "print(cs.execute(\"SELECT COUNT(*) FROM TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN\").fetchone()[0])\n",
    "for r in cs.execute(\"\"\"\n",
    "  SELECT SUPPLIERID, SUPPLIERNAME, PHONENUMBER, WEBSITEURL, VALIDFROM, VALIDTO\n",
    "  FROM TESTDB.PUBLIC.SUPPLIER_CASE_CLEAN\n",
    "  ORDER BY SUPPLIERID\n",
    "  LIMIT 10\n",
    "\"\"\").fetchall():\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "def find_weather_db(cs):\n",
    "    # Get all db names\n",
    "    names = [r[1] for r in cs.execute(\"SHOW DATABASES\").fetchall()]\n",
    "    # Try exact names from the brief\n",
    "    for cand in (\"WEATHER__ENVIRONMENT\", \"WEATHER_ENVIRONMENT\"):\n",
    "        if cand in names:\n",
    "            return cand\n",
    "    # Fuzzy fallback (handles custom names)\n",
    "    for n in names:\n",
    "        if \"WEATHER\" in n and \"ENVIRONMENT\" in n:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def print_table_sample(cs, fqtn, sample_rows=5):\n",
    "    print(f\"\\n=== {fqtn} ===\")\n",
    "    cs.execute(f\"SELECT * FROM {fqtn} LIMIT {sample_rows}\")\n",
    "    rows = cs.fetchall()\n",
    "    cols = [d[0] for d in cs.description]\n",
    "    print(\"Columns:\", \", \".join(cols))\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        print(f\"{i:>2}: {r}\")\n",
    "    cs.execute(f\"SELECT COUNT(*) FROM {fqtn}\")\n",
    "    print(\"Total rows:\", cs.fetchone()[0])\n",
    "\n",
    "def print_cybersyn_weather_tables(conn, warehouse=\"COMPUTE_WH\"):\n",
    "    with conn.cursor() as cs:\n",
    "        cs.execute(f\"USE WAREHOUSE {warehouse}\")\n",
    "\n",
    "        db_name = find_weather_db(cs)\n",
    "        if not db_name:\n",
    "            print(\"⚠️  Skipping Cybersyn weather: no WEATHER…ENVIRONMENT database found in this account.\")\n",
    "            have = [r[1] for r in cs.execute(\"SHOW DATABASES\").fetchall()]\n",
    "            print(\"Databases you have:\", have)\n",
    "            return\n",
    "\n",
    "        cs.execute(f\"USE DATABASE {db_name}\")\n",
    "\n",
    "        # Prefer CYBERSYN schema if present; otherwise fall back to PUBLIC\n",
    "        schemas = {r[1] for r in cs.execute(f\"SHOW SCHEMAS IN DATABASE {db_name}\").fetchall()}\n",
    "        schema = \"CYBERSYN\" if \"CYBERSYN\" in schemas else \"PUBLIC\"\n",
    "        cs.execute(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "        # If the exact NOAA table names differ, list what’s there and pick two NOAA* tables\n",
    "        all_tables = [r[1] for r in cs.execute(f\"SHOW TABLES IN SCHEMA {db_name}.{schema}\").fetchall()]\n",
    "        candidates = [t for t in all_tables if t.startswith(\"NOAA_\")]\n",
    "        if not candidates:\n",
    "            print(f\"No NOAA_* tables in {db_name}.{schema}. Available tables:\", all_tables)\n",
    "            return\n",
    "\n",
    "        for t in candidates[:2]:\n",
    "            print_table_sample(cs, f\"{db_name}.{schema}.{t}\", sample_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    ")\n",
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data\n",
      "Matched CSVs: 41\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-1.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-6.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-8.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-7.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-11.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2022-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-5.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-12.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-4.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-10.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-9.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2021-2.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2020-3.csv\n",
      "PUT -> file:///home/jovyan/UCSD CLASSES/MGTA 464- SQL/MGTA_464_Snowflake_Project/Data/Monthly PO Data/2019-1.csv\n",
      "Staged objects (top 10): [('po_data_stage/2019-1.csv.gz', 6784, 'a15f2fa77d98a431c564917c46a8f54f', 'Sun, 14 Sep 2025 21:57:56 GMT'), ('po_data_stage/2019-10.csv.gz', 3328, '30fddf0b054c15abdf58d9008cfd507d', 'Sun, 14 Sep 2025 21:57:42 GMT'), ('po_data_stage/2019-11.csv.gz', 3056, 'cea00f90062fda98ac3b9cf93003f446', 'Sun, 14 Sep 2025 21:57:44 GMT'), ('po_data_stage/2019-12.csv.gz', 2992, 'f7f2bf307ed9bc4844dac53cbd45e6e6', 'Sun, 14 Sep 2025 21:57:53 GMT'), ('po_data_stage/2019-2.csv.gz', 2384, 'c63aedaf830d7dba4e7080fcd8b2fd8a', 'Sun, 14 Sep 2025 21:57:40 GMT'), ('po_data_stage/2019-3.csv.gz', 2848, 'dc2ccfb5cad11094f2139c3bd04bfb5e', 'Sun, 14 Sep 2025 21:57:43 GMT'), ('po_data_stage/2019-4.csv.gz', 3088, '8c7471de208492d2a57559e77a1b148d', 'Sun, 14 Sep 2025 21:57:46 GMT'), ('po_data_stage/2019-5.csv.gz', 3200, '8c01985520ab970c77732bd8a2d88a9d', 'Sun, 14 Sep 2025 21:57:44 GMT'), ('po_data_stage/2019-6.csv.gz', 3024, 'f926b36f2e78f5ec00e1b688260f5d9e', 'Sun, 14 Sep 2025 21:57:31 GMT'), ('po_data_stage/2019-7.csv.gz', 3264, 'ec2a86136f0ff8bf13eeeac1ded84faa', 'Sun, 14 Sep 2025 21:57:51 GMT')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd100688f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "\n",
    "## Creating the PO_Table with Datatypes \n",
    "cs.execute(f\"USE DATABASE {DB}\")\n",
    "cs.execute(f\"USE SCHEMA {SCHEMA}\")\n",
    "cs.execute(\n",
    "\"CREATE OR REPLACE TABLE PO_Data(\"\n",
    "\"purchaseorderid NUMBER(38,0), \"\n",
    "\"supplierid NUMBER(38,0), \"\n",
    "\"orderdate DATE, \"\n",
    "\"deliverymethodid NUMBER(38,0), \"\n",
    "\"contactpersonid NUMBER(38,0), \"\n",
    "\"expecteddeliverydate DATE, \"\n",
    "\"supplierreference VARCHAR, \"\n",
    "\"isorderfinalized NUMBER(1,0), \"\n",
    "\"comments VARCHAR, \"\n",
    "\"internalcomments VARCHAR, \"\n",
    "\"lasteditedby NUMBER(38,0), \"\n",
    "\"purchaseorderlineid NUMBER(38,0), \"\n",
    "\"stockitemid NUMBER(38,0), \"\n",
    "\"orderedouters NUMBER(38,0), \"\n",
    "\"description VARCHAR, \"\n",
    "\"receivedouters NUMBER(38,0), \"\n",
    "\"packagetypeid NUMBER(38,0), \"\n",
    "\"expectedunitpriceperouter NUMBER(18,4), \"\n",
    "\"lastreceiptdate DATE, \"\n",
    "\"isorderlinefinalized NUMBER(1,0), \"\n",
    "\"right_lasteditedby NUMBER(38,0), \"\n",
    "\"right_lasteditedwhen TIMESTAMP_NTZ\"\n",
    "\")\")\n",
    "\n",
    "# ---------- Resolve repo-relative data folder ----------\n",
    "def find_monthly_po_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Locate the 'Data/Monthly PO Data' folder relative to the repository.\n",
    "    Works from notebooks or scripts, on Windows/macOS/Linux.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        Path.cwd() / \"Data\" / \"Monthly PO Data\",\n",
    "        Path.cwd() / \"data\" / \"Monthly PO Data\",\n",
    "    ]\n",
    "\n",
    "    # If running from a subfolder, search upward then rglob for the directory\n",
    "    # 1) Walk up to (at most) 5 levels to find a '.git' folder (repo root)\n",
    "    here = Path.cwd()\n",
    "    ups = [here] + list(here.parents)[:5]\n",
    "    repo_roots = [p for p in ups if (p / \".git\").exists()]\n",
    "    roots_to_search = repo_roots[:1] or [here]\n",
    "\n",
    "    for root in roots_to_search:\n",
    "        candidates.append(root / \"Data\" / \"Monthly PO Data\")\n",
    "        candidates.append(root / \"data\" / \"Monthly PO Data\")\n",
    "        # fallback: recursive search for the exact folder name\n",
    "        for p in root.rglob(\"Monthly PO Data\"):\n",
    "            candidates.append(p)\n",
    "\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            # Must contain CSVs to be considered valid\n",
    "            if any(p.glob(\"*.csv\")):\n",
    "                return p\n",
    "\n",
    "    raise SystemExit(\"Could not find 'Data/Monthly PO Data' in this repo. \"\n",
    "                     \"Make sure the data folder exists and contains .csv files.\")\n",
    "\n",
    "local_dir_path = find_monthly_po_dir()\n",
    "local_dir = str(local_dir_path)  # keep your existing code style\n",
    "print(\"Using data folder:\", local_dir)\n",
    "\n",
    "# ---------- Stage + file format ----------\n",
    "\n",
    "\n",
    "cs.execute(\"CREATE OR REPLACE STAGE po_data_stage\")\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT po_csv_ff\n",
    "  TYPE=CSV\n",
    "  FIELD_DELIMITER=','\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "  SKIP_HEADER=1\n",
    "  TRIM_SPACE=TRUE\n",
    "  EMPTY_FIELD_AS_NULL=TRUE\n",
    "  NULL_IF=('','NULL','null','00:00.0','0:00.0','00:00','0:00')\n",
    "  DATE_FORMAT='AUTO'\n",
    "  TIME_FORMAT='AUTO'\n",
    "  TIMESTAMP_FORMAT='AUTO'\n",
    "\"\"\")\n",
    "\n",
    "# ---------- Local files to stage (repo-relative) ----------\n",
    "\n",
    "pattern = os.path.join(local_dir, \"*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "print(\"Matched CSVs:\", len(files))\n",
    "if not files:\n",
    "    raise SystemExit(f\"No CSVs matched at: {pattern}\")\n",
    "\n",
    "# ---------- PUT files into stage (auto-compress -> .gz) ----------\n",
    "for filepath in files:\n",
    "    base = os.path.basename(filepath)\n",
    "    if \":\" in base:   # skip Windows ADS like ':Zone.Identifier'\n",
    "        continue\n",
    "    abs_path = os.path.abspath(filepath).replace(\"\\\\\", \"/\")   # ensure forward slashes\n",
    "    file_uri = \"file:///\" + abs_path.lstrip(\"/\")              # exactly 3 slashes, no URL-encoding\n",
    "    print(\"PUT ->\", file_uri)\n",
    "    cs.execute(f\"PUT '{file_uri}' @po_data_stage AUTO_COMPRESS=TRUE OVERWRITE=TRUE\")\n",
    "    \n",
    "\n",
    "# --- sanity check what's in the stage ---\n",
    "cs.execute(\"LIST @po_data_stage\")\n",
    "print(\"Staged objects (top 10):\", cs.fetchall()[:10])\n",
    "\n",
    "# --- load into the table (skipping $12 = lasteditedwhen) ---\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PO_Data\n",
    "  FROM (\n",
    "    SELECT\n",
    "      $1  ::NUMBER(38,0)  AS purchaseorderid,\n",
    "      $2  ::NUMBER(38,0)  AS supplierid,\n",
    "      TRY_TO_DATE($3)     AS orderdate,\n",
    "      $4  ::NUMBER(38,0)  AS deliverymethodid,\n",
    "      $5  ::NUMBER(38,0)  AS contactpersonid,\n",
    "      TRY_TO_DATE($6)     AS expecteddeliverydate,\n",
    "      $7                  AS supplierreference,\n",
    "      $8  ::NUMBER(1,0)   AS isorderfinalized,\n",
    "      $9                  AS comments,\n",
    "      $10                 AS internalcomments,\n",
    "      $11 ::NUMBER(38,0)  AS lasteditedby,\n",
    "      /* skip $12 */\n",
    "      $13 ::NUMBER(38,0)  AS purchaseorderlineid,\n",
    "      $14 ::NUMBER(38,0)  AS stockitemid,\n",
    "      $15 ::NUMBER(38,0)  AS orderedouters,\n",
    "      $16                 AS description,\n",
    "      $17 ::NUMBER(38,0)  AS receivedouters,\n",
    "      $18 ::NUMBER(38,0)  AS packagetypeid,\n",
    "      $19 ::NUMBER(18,4)  AS expectedunitpriceperouter,\n",
    "      TRY_TO_DATE($20)    AS lastreceiptdate,\n",
    "      $21 ::NUMBER(1,0)   AS isorderlinefinalized,\n",
    "      $22 ::NUMBER(38,0)  AS right_lasteditedby,\n",
    "      TRY_TO_TIMESTAMP_NTZ($23) AS right_lasteditedwhen\n",
    "    FROM @po_data_stage (FILE_FORMAT => 'po_csv_ff')\n",
    "  )\n",
    "  ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 8367\n",
      "Row count should equal 8367\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"SELECT COUNT(*) FROM PO_Data\")\n",
    "print(\"Row count:\", cs.fetchone()[0])\n",
    "print(\"Row count should equal 8367\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2160, 77, 767, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 767, 6, Decimal('84.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2161, 78, 981, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 981, 6, Decimal('84.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2162, 80, 397, '\"The Gu\" red shirt XML tag t-shirt (White) M', 397, 6, Decimal('84.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2163, 86, 816, '\"The Gu\" red shirt XML tag t-shirt (White) 5XL', 816, 6, Decimal('96.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2164, 95, 521, '\"The Gu\" red shirt XML tag t-shirt (Black) XL', 521, 6, Decimal('90.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(558, 4, datetime.date(2019, 12, 2), 7, 2, datetime.date(2019, 12, 22), '293092', 1, None, None, 8, 2165, 98, 978, '\"The Gu\" red shirt XML tag t-shirt (Black) 4XL', 978, 6, Decimal('96.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(559, 7, datetime.date(2019, 12, 2), 2, 2, datetime.date(2019, 12, 22), 'BC0280982', 1, None, None, 8, 2166, 193, 344, 'Black and orange glass with care despatch tape 48mmx75m', 344, 7, Decimal('38.4000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(559, 7, datetime.date(2019, 12, 2), 2, 2, datetime.date(2019, 12, 22), 'BC0280982', 1, None, None, 8, 2167, 204, 467, 'Tape dispenser (Red)', 467, 7, Decimal('170.0000'), datetime.date(2019, 12, 3), 1, 8, None)\n",
      "(560, 4, datetime.date(2019, 12, 3), 7, 2, datetime.date(2019, 12, 23), '293092', 1, None, None, 17, 2168, 77, 769, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 769, 6, Decimal('84.0000'), datetime.date(2019, 12, 4), 1, 17, None)\n",
      "(560, 4, datetime.date(2019, 12, 3), 7, 2, datetime.date(2019, 12, 23), '293092', 1, None, None, 17, 2169, 78, 979, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 979, 6, Decimal('84.0000'), datetime.date(2019, 12, 4), 1, 17, None)\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"SELECT * FROM PO_Data LIMIT 10\")\n",
    "for row in cs.fetchall():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) PO totals (POAmount) and a tidy PO header table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x73cd100688f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One row per purchase order with the required total\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PO_Header AS\n",
    "SELECT\n",
    "  purchaseorderid,\n",
    "  MIN(orderdate)                 AS orderdate,\n",
    "  MIN(supplierid)                AS supplierid,\n",
    "  SUM(receivedouters * expectedunitpriceperouter) AS POAmount\n",
    "FROM PO_Data\n",
    "GROUP BY purchaseorderid;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file URI: file:///home/jovyan/sf_uploads/supplier_transactions.xml\n",
      "[('invoice_xml_stage/supplier_transactions.xml.gz', 72528, '15c0f8f4276bf24ac457fb4e00e4107a', 'Sun, 14 Sep 2025 22:40:45 GMT')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Path to the XML in your repo (adjust this if your folder name differs)\n",
    "repo_xml = Path(\"Data\") / \"Supplier Transactions XML.xml\"\n",
    "\n",
    "# Make a simple, safe upload path that definitely exists *inside the container*\n",
    "safe_dir = Path.home() / \"sf_uploads\"\n",
    "safe_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "safe = safe_dir / \"supplier_transactions.xml\"   # normalize name\n",
    "shutil.copy2(repo_xml, safe)                     # copy into place\n",
    "\n",
    "uri = safe.as_uri()  # e.g., file:///home/jovyan/sf_uploads/supplier_transactions.xml\n",
    "print(\"Local file URI:\", uri)\n",
    "\n",
    "# Make sure the stage exists and is an *internal* stage\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS invoice_xml_stage\")\n",
    "\n",
    "# Do the PUT\n",
    "cs.execute(f\"PUT '{uri}' @invoice_xml_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "\n",
    "# Confirm\n",
    "print(cs.execute(\"LIST @invoice_xml_stage\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml_ff (with STRIP_OUTER_ELEMENT=TRUE) ready.\n"
     ]
    }
   ],
   "source": [
    "# 1) Recreate the XML file format WITH strip_outer_element\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT xml_ff\n",
    "  TYPE=XML\n",
    "  STRIP_OUTER_ELEMENT=TRUE\n",
    "\"\"\")\n",
    "print(\"xml_ff (with STRIP_OUTER_ELEMENT=TRUE) ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOICE_RAW rows (should be ~2438): 2438\n"
     ]
    }
   ],
   "source": [
    "# 2) Reload the XML so each <row> is its own table row\n",
    "cs.execute(\"CREATE OR REPLACE TABLE INVOICE_RAW (doc VARIANT)\")\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO INVOICE_RAW\n",
    "FROM @invoice_xml_stage\n",
    "FILE_FORMAT = xml_ff\n",
    "ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")\n",
    "print(\"INVOICE_RAW rows (should be ~2438):\", cs.execute(\"SELECT COUNT(*) FROM INVOICE_RAW\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in SUPPLIER_INVOICES: 2438\n",
      "[(134, 2, 5, 1, '7290', datetime.date(2019, 1, 2), Decimal('313.50'), Decimal('47.03'), Decimal('360.53'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (169, 4, 5, 2, '3898', datetime.date(2019, 1, 2), Decimal('21732.00'), Decimal('3259.80'), Decimal('24991.80'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (186, 5, 5, 3, '616', datetime.date(2019, 1, 2), Decimal('2740.50'), Decimal('411.11'), Decimal('3151.61'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (215, 7, 5, 4, '3869', datetime.date(2019, 1, 2), Decimal('42481.20'), Decimal('6372.19'), Decimal('48853.39'), Decimal('0.00'), datetime.date(2019, 1, 7), True), (224, 10, 5, 5, '4697', datetime.date(2019, 1, 2), Decimal('35067.50'), Decimal('5260.14'), Decimal('40327.64'), Decimal('0.00'), datetime.date(2019, 1, 7), True)]\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE SUPPLIER_INVOICES AS\n",
    "SELECT\n",
    "  TRY_TO_NUMBER(XMLGET(doc,'SupplierTransactionID'):\"$\"::STRING)      AS SupplierTransactionID,\n",
    "  TRY_TO_NUMBER(XMLGET(doc,'SupplierID'):\"$\"::STRING)                  AS SupplierID,\n",
    "  TRY_TO_NUMBER(XMLGET(doc,'TransactionTypeID'):\"$\"::STRING)           AS TransactionTypeID,\n",
    "  NULLIF(XMLGET(doc,'PurchaseOrderID'):\"$\"::STRING,'')::NUMBER         AS PurchaseOrderID,\n",
    "  NULLIF(XMLGET(doc,'SupplierInvoiceNumber'):\"$\"::STRING,'')           AS SupplierInvoiceNumber,\n",
    "  TRY_TO_DATE(XMLGET(doc,'TransactionDate'):\"$\"::STRING)               AS TransactionDate,   -- safer\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'AmountExcludingTax'):\"$\"::STRING,18,2)    AS AmountExcludingTax,\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'TaxAmount'):\"$\"::STRING,18,2)             AS TaxAmount,\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'TransactionAmount'):\"$\"::STRING,18,2)     AS TransactionAmount,\n",
    "  TRY_TO_DECIMAL(XMLGET(doc,'OutstandingBalance'):\"$\"::STRING,18,2)    AS OutstandingBalance,\n",
    "  TRY_TO_DATE(XMLGET(doc,'FinalizationDate'):\"$\"::STRING)              AS FinalizationDate,\n",
    "  TRY_TO_BOOLEAN(XMLGET(doc,'IsFinalized'):\"$\"::STRING)                AS IsFinalized\n",
    "FROM INVOICE_RAW\n",
    "\"\"\")\n",
    "\n",
    "print(\"Rows in SUPPLIER_INVOICES:\", cs.execute(\"SELECT COUNT(*) FROM SUPPLIER_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM SUPPLIER_INVOICES LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate PO tables: [('PUBLIC', 'PO_DATA')]\n",
      "Chosen PO table: ('PUBLIC', 'PO_DATA') with columns: RECEIVEDOUTERS x EXPECTEDUNITPRICEPEROUTER\n"
     ]
    }
   ],
   "source": [
    "# 1) Find a likely PO table and the columns we need\n",
    "#    (searches current database across common schemas; tweak the schema list if needed)\n",
    "schemas_to_check = [\"STAGE_AND_RAW\", \"PUBLIC\", \"RAW\", \"DATA\", \"MARTS\"]\n",
    "like_filters = [\"%PO%\", \"%PURCHASE%\"]\n",
    "\n",
    "# helper: run a query and return rows\n",
    "def q(sql):\n",
    "    return cs.execute(sql).fetchall()\n",
    "\n",
    "# find candidate tables\n",
    "candidates = []\n",
    "for sch in schemas_to_check:\n",
    "    rows = q(f\"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_type='BASE TABLE'\n",
    "          AND table_schema = '{sch}'\n",
    "          AND (UPPER(table_name) LIKE '{like_filters[0]}' OR UPPER(table_name) LIKE '{like_filters[1]}')\n",
    "    \"\"\")\n",
    "    candidates.extend(rows)\n",
    "\n",
    "print(\"Candidate PO tables:\", candidates)\n",
    "\n",
    "# choose the best candidate that has PurchaseOrderID and a price*qty pair\n",
    "chosen = None\n",
    "qty_col = None\n",
    "price_col = None\n",
    "\n",
    "pairs = [\n",
    "    (\"RECEIVEDOUTERS\", \"EXPECTEDUNITPRICEPEROUTER\"),\n",
    "    (\"QUANTITY\", \"UNITPRICE\")\n",
    "]\n",
    "\n",
    "for sch, tbl in candidates:\n",
    "    cols = {r[0] for r in q(f\"\"\"\n",
    "        SELECT UPPER(column_name)\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{sch}' AND table_name = '{tbl}'\n",
    "    \"\"\")}\n",
    "    if \"PURCHASEORDERID\" in cols:\n",
    "        for qcol, pcol in pairs:\n",
    "            if qcol in cols and pcol in cols:\n",
    "                chosen = (sch, tbl)\n",
    "                qty_col, price_col = qcol, pcol\n",
    "                break\n",
    "    if chosen:\n",
    "        break\n",
    "\n",
    "if not chosen:\n",
    "    raise RuntimeError(\"Couldn't auto-detect a PO detail table with the needed columns. \"\n",
    "                       \"If you already know it, set `chosen=('YOUR_SCHEMA','YOUR_TABLE')` \"\n",
    "                       \"and `qty_col, price_col` accordingly and re-run.\")\n",
    "\n",
    "print(\"Chosen PO table:\", chosen, \"with columns:\", qty_col, \"x\", price_col)\n",
    "\n",
    "# for convenience\n",
    "PO_SCHEMA, PO_TABLE = chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "po_totals sample: [(562, Decimal('402192.0000')), (576, Decimal('407994.0000')), (814, Decimal('186559.6000')), (577, Decimal('98336.4000')), (581, Decimal('413298.0000'))]\n"
     ]
    }
   ],
   "source": [
    "PO_SCHEMA, PO_TABLE = \"PUBLIC\", \"PO_DATA\"   # from your auto-detect result\n",
    "qty_col, price_col = \"RECEIVEDOUTERS\", \"EXPECTEDUNITPRICEPEROUTER\"\n",
    "\n",
    "cs.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {PO_SCHEMA}.PO_TOTALS AS\n",
    "SELECT\n",
    "  PurchaseOrderID,\n",
    "  SUM(COALESCE({qty_col},0) * COALESCE({price_col},0)) AS POAmount\n",
    "FROM {PO_SCHEMA}.{PO_TABLE}\n",
    "GROUP BY 1\n",
    "\"\"\")\n",
    "print(\"po_totals sample:\", cs.execute(f\"SELECT * FROM {PO_SCHEMA}.PO_TOTALS LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "002003 (42S02): SQL compilation error:\nView 'TESTDB.PUBLIC.PURCHASE_ORDERS_AND_INVOICES' does not exist or not authorized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Drop the view and create MV with the same name\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cs\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE SCHEMA PUBLIC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP VIEW PUBLIC.PURCHASE_ORDERS_AND_INVOICES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/snowflake/connector/cursor.py:1134\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1131\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1134\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/snowflake/connector/errors.py:286\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    265\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    294\u001b[0m             error_class,\n\u001b[1;32m    295\u001b[0m             error_value,\n\u001b[1;32m    296\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/snowflake/connector/errors.py:341\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 341\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/snowflake/connector/errors.py:217\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    215\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    218\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    219\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    220\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    221\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    222\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    225\u001b[0m     ),\n\u001b[1;32m    226\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    227\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    228\u001b[0m )\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 002003 (42S02): SQL compilation error:\nView 'TESTDB.PUBLIC.PURCHASE_ORDERS_AND_INVOICES' does not exist or not authorized."
     ]
    }
   ],
   "source": [
    "# Drop the view and create MV with the same name\n",
    "cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "\n",
    "cs.execute(\"DROP VIEW PUBLIC.PURCHASE_ORDERS_AND_INVOICES\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built PUBLIC.PURCHASE_ORDERS_AND_INVOICES as TABLE.\n",
      "Row count: 2072\n",
      "[(83481, 4, 562, datetime.date(2019, 12, 5), Decimal('402192.00'), Decimal('402192.0000'), Decimal('0.0000')), (85554, 4, 576, datetime.date(2019, 12, 16), Decimal('407994.00'), Decimal('407994.0000'), Decimal('0.0000')), (121887, 7, 814, datetime.date(2020, 5, 7), Decimal('186559.60'), Decimal('186559.6000'), Decimal('0.0000')), (85557, 7, 577, datetime.date(2019, 12, 16), Decimal('98336.40'), Decimal('98336.4000'), Decimal('0.0000')), (86182, 4, 581, datetime.date(2019, 12, 18), Decimal('413298.00'), Decimal('413298.0000'), Decimal('0.0000')), (86385, 4, 583, datetime.date(2019, 12, 19), Decimal('413682.00'), Decimal('413682.0000'), Decimal('0.0000')), (87795, 7, 594, datetime.date(2019, 12, 25), Decimal('102054.80'), Decimal('102054.8000'), Decimal('0.0000')), (123490, 4, 821, datetime.date(2020, 5, 13), Decimal('569754.00'), Decimal('569754.0000'), Decimal('0.0000')), (123675, 4, 823, datetime.date(2020, 5, 14), Decimal('569790.00'), Decimal('569790.0000'), Decimal('0.0000')), (124573, 7, 828, datetime.date(2020, 5, 16), Decimal('189055.20'), Decimal('189055.2000'), Decimal('0.0000'))]\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're in the right place (safe to re-run)\n",
    "cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "\n",
    "# Try MATERIALIZED VIEW first; fall back to TABLE if not supported\n",
    "built = \"MATERIALIZED VIEW\"\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE MATERIALIZED VIEW PUBLIC.PURCHASE_ORDERS_AND_INVOICES AS\n",
    "    SELECT\n",
    "      si.SupplierTransactionID,\n",
    "      si.SupplierID,\n",
    "      si.PurchaseOrderID,\n",
    "      si.TransactionDate                 AS InvoiceDate,\n",
    "      si.AmountExcludingTax,\n",
    "      pt.POAmount,\n",
    "      (si.AmountExcludingTax - pt.POAmount) AS invoiced_vs_quoted\n",
    "    FROM PUBLIC.SUPPLIER_INVOICES si\n",
    "    JOIN PUBLIC.PO_TOTALS pt USING (PurchaseOrderID)\n",
    "    \"\"\")\n",
    "except Exception:\n",
    "    built = \"TABLE\"\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE PUBLIC.PURCHASE_ORDERS_AND_INVOICES AS\n",
    "    SELECT\n",
    "      si.SupplierTransactionID,\n",
    "      si.SupplierID,\n",
    "      si.PurchaseOrderID,\n",
    "      si.TransactionDate                 AS InvoiceDate,\n",
    "      si.AmountExcludingTax,\n",
    "      pt.POAmount,\n",
    "      (si.AmountExcludingTax - pt.POAmount) AS invoiced_vs_quoted\n",
    "    FROM PUBLIC.SUPPLIER_INVOICES si\n",
    "    JOIN PUBLIC.PO_TOTALS pt USING (PurchaseOrderID)\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"Built PUBLIC.PURCHASE_ORDERS_AND_INVOICES as {built}.\")\n",
    "print(\"Row count:\", cs.execute(\"SELECT COUNT(*) FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES LIMIT 10\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If host is macOS/Windows this works out of the box:\n",
    "PG_HOST = \"host.docker.internal\"\n",
    "\n",
    "# If you're on Linux and host.docker.internal doesn't resolve, uncomment this instead:\n",
    "# import subprocess\n",
    "# PG_HOST = subprocess.check_output(\"ip route | awk '/default/ {print $3}'\", shell=True).decode().strip()\n",
    "\n",
    "PG_PORT = 8765            # <-- from your VS Code connection\n",
    "PG_DB   = \"rsm-docker\"    # <-- from your VS Code connection\n",
    "PG_USER = \"jovyan\"        # <-- from your VS Code connection\n",
    "PG_PWD  = \"postgres\"      # <-- from your VS Code connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Postgres at host.docker.internal 8765\n",
      "Tables named supplier_case: [('public', 'supplier_case')]\n",
      "Columns: ['supplierid', 'suppliername', 'suppliercategoryid', 'primarycontactpersonid', 'alternatecontactpersonid', 'deliverymethodid', 'postalcityid', 'supplierreference', 'bankaccountname', 'bankaccountbranch', 'bankaccountcode', 'bankaccountnumber', 'bankinternationalcode', 'paymentdays', 'internalcomments', 'phonenumber', 'faxnumber', 'websiteurl', 'deliveryaddressline1', 'deliveryaddressline2', 'deliverypostalcode', 'deliverylocation', 'postaladdressline1', 'postaladdressline2', 'postalpostalcode', 'lasteditedby', 'validfrom', 'validto']\n",
      "Row count: 13\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=PG_HOST, port=PG_PORT, dbname=PG_DB, user=PG_USER, password=PG_PWD)\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    print(\"✅ Connected to Postgres at\", PG_HOST, PG_PORT)\n",
    "\n",
    "    # Find the table (unquoted identifiers in Postgres become lowercase)\n",
    "    cur.execute(\"\"\"\n",
    "      SELECT table_schema, table_name\n",
    "      FROM information_schema.tables\n",
    "      WHERE table_name ILIKE 'supplier_case'\n",
    "    \"\"\")\n",
    "    print(\"Tables named supplier_case:\", cur.fetchall())\n",
    "\n",
    "    # Show columns (you’ll likely see supplierid, suppliername, postalpostalcode)\n",
    "    cur.execute(\"\"\"\n",
    "      SELECT column_name\n",
    "      FROM information_schema.columns\n",
    "      WHERE table_name ILIKE 'supplier_case'\n",
    "      ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    cols = [r[0] for r in cur.fetchall()]\n",
    "    print(\"Columns:\", cols)\n",
    "\n",
    "    # Quick row count\n",
    "    cur.execute('SELECT COUNT(*) FROM public.supplier_case')\n",
    "    print(\"Row count:\", cur.fetchone()[0])\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"❌ Postgres connection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported CSV: /home/jovyan/sf_uploads/supplier_case_export.csv\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pathlib import Path\n",
    "\n",
    "conn = psycopg2.connect(host=PG_HOST, port=PG_PORT, dbname=PG_DB, user=PG_USER, password=PG_PWD)\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "safe_dir = Path.home() / \"sf_uploads\"\n",
    "safe_dir.mkdir(parents=True, exist_ok=True)\n",
    "export_csv = safe_dir / \"supplier_case_export.csv\"\n",
    "\n",
    "# Use lowercase column names per Postgres rules\n",
    "with open(export_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    cur.copy_expert(\n",
    "        \"\"\"\n",
    "        COPY (\n",
    "            SELECT supplierid, suppliername, postalpostalcode\n",
    "            FROM public.supplier_case\n",
    "        ) TO STDOUT WITH CSV HEADER\n",
    "        \"\"\",\n",
    "        f\n",
    "    )\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Exported CSV:\", export_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged: [('refdata_stage/supplier_case_export.csv.gz', 320, 'afd5053a5689ecf4cc02688db7c733da', 'Sun, 14 Sep 2025 23:46:14 GMT')]\n",
      "Rows in PUBLIC.SUPPLIER_CASE: 13\n",
      "[('1', 'A Datum Corporation', '22202'), ('2', 'Contoso, Ltd.', '80125'), ('3', 'Consolidated Messenger', '60523'), ('4', 'Fabrikam, Inc.', '95642'), ('5', 'Graphic Design Institute', '80125')]\n"
     ]
    }
   ],
   "source": [
    "# Use your existing Snowflake cursor `cs`\n",
    "uri = export_csv.resolve().as_uri()\n",
    "\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS PUBLIC.refdata_stage\")\n",
    "cs.execute(f\"PUT '{uri}' @PUBLIC.refdata_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "print(\"Staged:\", cs.execute(\"LIST @PUBLIC.refdata_stage\").fetchall())\n",
    "\n",
    "# Keep STRING types to preserve leading zeros in ZIPs\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.SUPPLIER_CASE (\n",
    "  SupplierID STRING,\n",
    "  SupplierName STRING,\n",
    "  PostalPostalCode STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PUBLIC.SUPPLIER_CASE\n",
    "FROM (SELECT $1, $2, $3 FROM @PUBLIC.refdata_stage)\n",
    "FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1)\n",
    "ON_ERROR=ABORT_STATEMENT\n",
    "\"\"\")\n",
    "\n",
    "print(\"Rows in PUBLIC.SUPPLIER_CASE:\",\n",
    "      cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_CASE\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.SUPPLIER_CASE LIMIT 5\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2438\n",
      "2074\n",
      "2072\n",
      "13\n",
      "[(83481, 4, 562, datetime.date(2019, 12, 5), Decimal('402192.00'), Decimal('402192.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (85554, 4, 576, datetime.date(2019, 12, 16), Decimal('407994.00'), Decimal('407994.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (121887, 7, 814, datetime.date(2020, 5, 7), Decimal('186559.60'), Decimal('186559.6000'), Decimal('0.0000'), 'Litware, Inc.', '95642'), (85557, 7, 577, datetime.date(2019, 12, 16), Decimal('98336.40'), Decimal('98336.4000'), Decimal('0.0000'), 'Litware, Inc.', '95642'), (86182, 4, 581, datetime.date(2019, 12, 18), Decimal('413298.00'), Decimal('413298.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (86385, 4, 583, datetime.date(2019, 12, 19), Decimal('413682.00'), Decimal('413682.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (87795, 7, 594, datetime.date(2019, 12, 25), Decimal('102054.80'), Decimal('102054.8000'), Decimal('0.0000'), 'Litware, Inc.', '95642'), (123490, 4, 821, datetime.date(2020, 5, 13), Decimal('569754.00'), Decimal('569754.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (123675, 4, 823, datetime.date(2020, 5, 14), Decimal('569790.00'), Decimal('569790.0000'), Decimal('0.0000'), 'Fabrikam, Inc.', '95642'), (124573, 7, 828, datetime.date(2020, 5, 16), Decimal('189055.20'), Decimal('189055.2000'), Decimal('0.0000'), 'Litware, Inc.', '95642')]\n"
     ]
    }
   ],
   "source": [
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.PO_TOTALS\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT COUNT(*) FROM PUBLIC.SUPPLIER_CASE\").fetchone()[0])\n",
    "\n",
    "# Join suppliers onto the PO/invoice view (zip normalized)\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PUBLIC.JOINED_WITH_SUPPLIERS AS\n",
    "SELECT\n",
    "  p.*,\n",
    "  s.SupplierName,\n",
    "  LPAD(REGEXP_REPLACE(s.PostalPostalCode,'\\\\D',''),5,'0') AS PostalPostalCode\n",
    "FROM PUBLIC.PURCHASE_ORDERS_AND_INVOICES p\n",
    "LEFT JOIN PUBLIC.SUPPLIER_CASE s\n",
    "  ON TO_VARCHAR(p.SupplierID) = TO_VARCHAR(s.SupplierID)\n",
    "\"\"\")\n",
    "print(cs.execute(\"SELECT * FROM PUBLIC.JOINED_WITH_SUPPLIERS LIMIT 10\").fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZCTA extracted: /home/jovyan/sf_uploads/2021_Gaz_zcta_national.txt\n",
      "Staged: [('zcta_stage/2021_Gaz_zcta_national.txt.gz', 966848, 'c040089a8850ac7d8e74238beacb9315', 'Mon, 15 Sep 2025 00:09:49 GMT')]\n",
      "ZCTA rows: 33791\n",
      "[('00601', 18.180555, -66.749961), ('00602', 18.361945, -67.175597), ('00603', 18.458497, -67.123906), ('00606', 18.158327, -66.932928), ('00610', 18.294032, -67.127156)]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# Unzip the file\n",
    "zcta_zip = Path(\"Data\") / \"2021_Gaz_zcta_national.zip\"\n",
    "safe_dir = Path.home() / \"sf_uploads\"\n",
    "safe_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zcta_zip, 'r') as zf:\n",
    "    member = [m for m in zf.namelist() if m.lower().endswith(\".txt\")][0]\n",
    "    zf.extract(member, safe_dir)\n",
    "    zcta_txt = safe_dir / member\n",
    "\n",
    "print(\"ZCTA extracted:\", zcta_txt)\n",
    "\n",
    "# Stage the file\n",
    "uri = zcta_txt.resolve().as_uri()\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS PUBLIC.zcta_stage\")\n",
    "cs.execute(f\"PUT '{uri}' @PUBLIC.zcta_stage OVERWRITE=TRUE AUTO_COMPRESS=TRUE\")\n",
    "print(\"Staged:\", cs.execute(\"LIST @PUBLIC.zcta_stage\").fetchall())\n",
    "\n",
    "# Create table for ZIP, lat, lon\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.ZCTA_2021 (\n",
    "  GEOID     STRING,\n",
    "  INTPTLAT  FLOAT,\n",
    "  INTPTLONG FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Define TSV file format (skip header, tab delimiter)\n",
    "# File format: tab-delimited, header row, values optionally quoted, trim spaces\n",
    "# Use the TSV format we already created\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT PUBLIC.TSV_FF\n",
    "  TYPE = CSV\n",
    "  FIELD_DELIMITER = '\\t'\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "  TRIM_SPACE = TRUE\n",
    "  SKIP_HEADER = 1\n",
    "  NULL_IF = ('','NULL')\n",
    "\"\"\")\n",
    "\n",
    "# Recreate the target table\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.ZCTA_2021 (\n",
    "  GEOID     STRING,\n",
    "  INTPTLAT  DOUBLE,\n",
    "  INTPTLONG DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# COPY using correct column positions: $1 (GEOID), $6 (LAT), $7 (LON)\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PUBLIC.ZCTA_2021 (GEOID, INTPTLAT, INTPTLONG)\n",
    "FROM (\n",
    "  SELECT\n",
    "    $1,\n",
    "    TRY_TO_DOUBLE(REPLACE($6,'+','')),\n",
    "    TRY_TO_DOUBLE(REPLACE($7,'+',''))\n",
    "  FROM @PUBLIC.zcta_stage (FILE_FORMAT => 'PUBLIC.TSV_FF')\n",
    ")\n",
    "ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")\n",
    "\n",
    "print(\"ZCTA rows:\", cs.execute(\"SELECT COUNT(*) FROM PUBLIC.ZCTA_2021\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT GEOID, INTPTLAT, INTPTLONG FROM PUBLIC.ZCTA_2021 ORDER BY GEOID LIMIT 5\").fetchall())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate clean table\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE PUBLIC.ZCTA_2021 (\n",
    "  GEOID     STRING,\n",
    "  INTPTLAT  DOUBLE,\n",
    "  INTPTLONG DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# COPY: $1=GEOID, $8=INTPTLAT, $9=INTPTLONG; remove leading '+' before casting\n",
    "cs.execute(\"\"\"\n",
    "COPY INTO PUBLIC.ZCTA_2021 (GEOID, INTPTLAT, INTPTLONG)\n",
    "FROM (\n",
    "  SELECT\n",
    "    $1,\n",
    "    TRY_TO_DOUBLE(REPLACE($8,'+','')),\n",
    "    TRY_TO_DOUBLE(REPLACE($9,'+',''))\n",
    "  FROM @PUBLIC.zcta_stage (FILE_FORMAT => 'PUBLIC.TSV_FF')\n",
    ")\n",
    "ON_ERROR = ABORT_STATEMENT\n",
    "\"\"\")\n",
    "\n",
    "print(\"ZCTA rows:\", cs.execute(\"SELECT COUNT(*) FROM PUBLIC.ZCTA_2021\").fetchone()[0])\n",
    "print(cs.execute(\"SELECT GEOID, INTPTLAT, INTPTLONG FROM PUBLIC.ZCTA_2021 ORDER BY GEOID LIMIT 5\").fetchall())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
